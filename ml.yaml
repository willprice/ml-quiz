title: Machine Learning Quiz
url: "http://willprice.github.io/ml-quiz/"
candidate_number:
  - "64496"
  - "54321"
"1":
  images: []
  problem_type: training
  difficulty: "3"
  reference: "1.2"
  source: "http://www.csd.uwo.ca/courses/CS9840a/Lecture2_knn.pdf"
  question: |
    Given the training data

    $$\mathbf{x} =
      \begin{pmatrix}
        3 & 3 \\
        1 & 0 \\
        1 & 2 \\
        2 & 3 \\
        2 & 1 \\
      \end{pmatrix}$$

    with the labels:

    $$\mathbf{y} =
    \begin{pmatrix}
        0 \\
        1 \\
        0 \\
        0 \\
        1 \\
      \end{pmatrix}$$

    where an instance belonging to the $\oplus$ class is represented by
    $1$ in $\mathbf{y}$, and an instance belonging to the $\ominus$
    class by $0$.

    Classify the test instance $(0, 1)$ using the 3 nearest neighbours
    classifier with Euclidean distances by calculation
  answer_type: single
  answers:
    - correctness: "+"
      explanation: "The nearest points to $(0, 1)$ are $(1, 0)$, $(1, 2)$ and $(2, 1)$"
      answer: "$\\oplus$"
    - correctness: "-"
      answer: "$\\ominus$"
  workings: |
    We need to find the distance between our test point and every point
    in the training data set.

    First we find the distance between the test point and training
    points in each dimension.

    $$\mathbf{x} - (0, 1) =
    \begin{pmatrix}
      3 & 2 \\
      1 & -1 \\
      1 & 1 \\
      2 & 2 \\
      2 & 0 \\
    \end{pmatrix}$$

    We can then apply our distance measure to work out the distances.

    $$\mathbf{D(x, (0, 1))} =
    \begin{pmatrix}
      \sqrt{(3^2 + 2^2)} \\
      \sqrt{(1^2 + {(-1)}^2)} \\
      \sqrt{(1^2 + 1^2)} \\
      \sqrt{(2^2 + 2^2)} \\
      \sqrt{(2^2 + 0^2)} \\
    \end{pmatrix}
    =
    \begin{pmatrix}
      3.6 \\
      1.4 \\
      1.4 \\
      2.8 \\
      2 \\
    \end{pmatrix}$$

    The 2nd, 3rd and 5th entries are the closest to the test point,
    looking this up in our training data vector $\mathbf{x}$ we see they
    correspond to the following points, $(1, 0)$, $(1, 2)$ and $(2, 1)$,
    which each have the classes $\oplus$, $\ominus$, $\oplus$ respectively.
    We take the majority class as the prediction for our test point, $\oplus$.
  hint: |
    First plot the data then see whether you can visually identify the
    points that are closest.
  comments: |
    Tests the student's knowledge of training and using a KNN model.
    <br>

    Since the student has to both train and test using classifier, a
    fair amount of calculation, this question scores a 3 in difficulty.


"2":
  image: []
  problem_type: problem-solving
  difficulty: "3"
  reference: "2.1"
  source: "https://en.wikipedia.org/wiki/Precision_and_recall"
  question: |
    Fill in the contingency table.
    <table>
      <tr>
        <th>Measure</th>
        <th>Value</th>
      </tr>
      <tr>
        <td>$\sum_{x \in Te}\left[ I(\hat{c}(x) = \oplus \right] = 45$</td>
        <td>45</td>
      </tr>
      <tr>
        <td>$\sum_{x \in Te}\left[ I(\hat{c}(x) = \ominus \right] = 20$</td>
        <td>20</td>
      </tr>
      <tr>
        <td>Precision</td>
        <td>$\frac{8}{9}$</td>
      </tr>
      <tr>
        <td>False positive rate</td>
        <td>$\frac{1}{4}$</td>
      </tr>
    </table>
  answer_type: cloze_answer
  answers:
    answer:
      - "40 | 5  | 45"
      - "------------"
      - "5  | 15 | 20"
      - "------------"
      - "45 | 20 | 65"
    explanation: See the workings for derivation
  workings: |
    Recall the following defintions
    $$\text{Precision} = \frac{TP}{TP + FP}$$
    $$\text{False positive rate} = \frac{FP}{Neg} = \frac{FP}{FP + TN}$$
    <br>

    <table>
      <thead>
        <tr>
          <th></th>
          <th>Predicted +</th>
          <th>Predicted -</th>
          <th>Marginals</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Actual +</td>
          <td>$TP$</td>
          <td>$FN$</td>
          <td>$TP + FN$</td>
        </tr>
        <tr>
          <td>Actual -</td>
          <td>$FP$</td>
          <td>$TN$</td>
          <td>$FP + TN$</td>
        </tr>
        <tr>
          <td>Marginals</td>
          <td>$TP + FP$</td>
          <td>$FN + TN$</td>
          <td>$|Te| = TP + FP + FN + TN$</td>
        </tr>
      </tbody>
    </table>

    We need to derive set of simultaneous equations describing the
    information given.

    $$
    \mathrm{precision} = \frac{8}{9} \Leftrightarrow \frac{TP}{TP + FP}
      = \frac{8}{9} \Leftrightarrow TP = 8 \cdot FP$$
    $$\mathrm{fpr} = \frac{1}{4} \Leftrightarrow \frac{FP}{FP + TN} = \frac{1}{4}
      \Leftrightarrow TN = 3 \cdot FP$$

    Using the above with the sums of predicted classes we have the
    following four equations, numbered 1 to 4 from top to bottom.

    $$
    \begin{align}
    TP + FP &= 45 \\
    TN + FN &= 20 \\
    TP &= 8 \cdot FP \\
    TN &= 3 \cdot FP \\
    \end{align}
    $$

    Substituting eq. 3 into eq. 1 yields $FP = 5$ and hence $TP = 40$,
    we then solve for $TN$ using eq. 4, $TN = 15$, finally solving for
    $FN$ using eq. 2, $FN = 5$

  hint: |
    What are the definitions of *precision* and *false positive rate*?
    Try to derive them in terms of the individual elements of the
    contingency table.
  comments: |
    Tests the student's knowledge of contingency tables: what does each
    cell mean, how are the marginals calculated. It also tests the
    definitions of precision and false positive rate in terms of true
    positives, true negatives, and false positives.
    <br>
    The additional twist of solving with simultaneous equations bumps
    the complexity up from a 2 to 3.
"3":
  image: []
  problem_type: problem-solving
  difficulty: "3"
  reference: "3.3"
  source: "URL"
  question:
    - |
      <p>
      We're trying to find out the characteristics of rodents, so a
      variety of animals were sampled and certain features about them
      were recorded. We'd like to come up with some conjunctive concepts
      to determine whether a certain animal is a rodent based on it's
      features.
      </p>

      <p>
      The examples below cover the full range of feature values, (i.e.
      there is no 'Very long' value for ear size)
      </p>

      <table>
      <thead>
        <tr>
          <th>Eye size</th>
          <th>Tail length</th>
          <th>Ear size</th>
          <th>Class</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <!-- mouse -->
          <td>Large</td>
          <td>Long</td>
          <td>Very Large</td>
          <td>+</td>
        </tr>
        <tr>
          <td>Small</td>
          <td>Short</td>
          <td>Large</td>
          <td>-</td>
        </tr>
        <tr>
          <!-- vole -->
          <td>Small</td>
          <td>Short</td>
          <td>Small</td>
          <td>+</td>
        </tr>
        <tr>
          <!-- shrew -->
          <td>Small</td>
          <td>Short</td>
          <td>Small</td>
          <td>+</td>
        </tr>
      
        <tr>
          <td>Small</td>
          <td>Long</td>
          <td>Large</td>
          <td>-</td>
        </tr>
        <tr>
          <!-- possum -->
          <td>Large</td>
          <td>Long</td>
          <td>Small</td>
          <td>+</td>
        </tr>
        <tr>
          <td>Small</td>
          <td>Short</td>
          <td>Large</td>
          <td>-</td>
        </tr>
        <tr>
          <td>Small</td>
          <td>Short</td>
          <td>Small</td>
          <td>-</td>
        </tr>
      </tbody>
      </table>

      <p>
      The following sets of hypotheses are defined
      <ul>
       <li>$A$, the set of complete hypotheses</li>
       <li>$B$, the set of consistent hypotheses, covering at least one positive example</li>
       <li>$V$, the version space</li>
      </ul>
    - |
      <br />
      What is $|A|$
    - 1
    - |
      <br />
      What is $|B|$
    - 2
    - |
      <br />
      What is $|V|$
    - 3
    - </p>

  answer_type: blank_answer
  answers:
    - correctness: 1
      answer: "2"
      explanation: The number of hypotheses that cover all the positive examples
    - correctness: 2
      answer: "16"
      explanation: |
        The number of hypotheses that cover none of the negative
        examples and at least one of the positive examples
    - correctness: 3
      answer: "0"
      explanation: There are no complete AND consistent hypotheses
  workings: |
    <p>
    The hypothesis space of all conjunctive concepts needs to be
    computed, then for each conjunctive concept we have to count the
    number of positive and negative examples, from that we can derive
    the size of each of the sets of hypotheses.
    </p>

    <p>
    First it's sensible to reoder the table to separate positive and
    negative examples to help ease counting.
    </p>

    <table>
    <thead>
      <tr>
        <th>Eye size</th>
        <th>Tail length</th>
        <th>Ear size</th>
        <th>Class</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <!-- mouse -->
        <td>Large</td>
        <td>Long</td>
        <td>Very Large</td>
        <td>+</td>
      </tr>
      <tr>
        <!-- vole -->
        <td>Small</td>
        <td>Short</td>
        <td>Small</td>
        <td>+</td>
      </tr>
      <tr>
        <!-- shrew -->
        <td>Small</td>
        <td>Short</td>
        <td>Small</td>
        <td>+</td>
      </tr>
      <tr>
        <!-- possum -->
        <td>Large</td>
        <td>Long</td>
        <td>Small</td>
        <td>+</td>
      </tr>

      <tr>
        <td>Small</td>
        <td>Short</td>
        <td>Large</td>
        <td>-</td>
      </tr>
      <tr>
        <td>Small</td>
        <td>Long</td>
        <td>Large</td>
        <td>-</td>
      </tr>
      <tr>
        <td>Small</td>
        <td>Short</td>
        <td>Large</td>
        <td>-</td>
      </tr>
      <tr>
        <td>Small</td>
        <td>Short</td>
        <td>Small</td>
        <td>-</td>
      </tr>
    </tbody>
    </table>

    <p>
      Each feature can take on one of several values, each conjunctive
      literal will use an internal disjunction of these values. The
      possible internal disjunctions are calculated as they are
      necessary to build the hypothesis space.

      We denote the internal disjuction of all possible values with '-'
      as this functions as a wildcard, a "don't care" value, since the
      feature can take on any value and we will still match the instance.
    </p>


    <p>
     Every possible hypothesis is computed by the combination over all
     the possible interal disjunctions of each feature, this forms
     something similar to a truth table and can be constructed in a
     similar fashion.
    </p>

    <table>
      <thead>
        <tr><th>Eye size</th><th>Tail length</th><th>Ear size</th><th>#Pos</th><th>#Neg</th></tr>
      </thead>
      <tbody> 
        <tr><td>-</td><td>-</td><td>-</td><td>4</td><td>4</td></tr>
         <tr><td>-</td><td>-</td><td>Small</td><td>3</td><td>1</td></tr>
         <tr><td>-</td><td>-</td><td>Large</td><td>0</td><td>3</td></tr>
         <tr><td>-</td><td>-</td><td>Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>-</td><td>-</td><td>Small, Large</td><td>3</td><td>4</td></tr>
         <tr><td>-</td><td>-</td><td>Small, Very Large</td><td>4</td><td>1</td></tr>
         <tr><td>-</td><td>-</td><td>Large, Very Large</td><td>1</td><td>3</td></tr>
         <tr><td>-</td><td>Long</td><td>-</td><td>2</td><td>1</td></tr>
         <tr><td>-</td><td>Long</td><td>Small</td><td>1</td><td>0</td></tr>
         <tr><td>-</td><td>Long</td><td>Large</td><td>0</td><td>1</td></tr>
         <tr><td>-</td><td>Long</td><td>Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>-</td><td>Long</td><td>Small, Large</td><td>1</td><td>1</td></tr>
         <tr><td>-</td><td>Long</td><td>Small, Very Large</td><td>2</td><td>0</td></tr>
         <tr><td>-</td><td>Long</td><td>Large, Very Large</td><td>1</td><td>1</td></tr>
         <tr><td>-</td><td>Short</td><td>-</td><td>2</td><td>3</td></tr>
         <tr><td>-</td><td>Short</td><td>Small</td><td>2</td><td>1</td></tr>
         <tr><td>-</td><td>Short</td><td>Large</td><td>0</td><td>2</td></tr>
         <tr><td>-</td><td>Short</td><td>Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>-</td><td>Short</td><td>Small, Large</td><td>2</td><td>3</td></tr>
         <tr><td>-</td><td>Short</td><td>Small, Very Large</td><td>2</td><td>1</td></tr>
         <tr><td>-</td><td>Short</td><td>Large, Very Large</td><td>0</td><td>2</td></tr>
         <tr><td>Large</td><td>-</td><td>-</td><td>2</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Small</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Small, Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Small, Very Large</td><td>2</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Large, Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>-</td><td>2</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Small</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Small, Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Small, Very Large</td><td>2</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Large, Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>-</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Small</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Small, Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Small, Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Large, Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>-</td><td>-</td><td>2</td><td>4</td></tr>
         <tr><td>Small</td><td>-</td><td>Small</td><td>2</td><td>1</td></tr>
         <tr><td>Small</td><td>-</td><td>Large</td><td>0</td><td>3</td></tr>
         <tr><td>Small</td><td>-</td><td>Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>-</td><td>Small, Large</td><td>2</td><td>4</td></tr>
         <tr><td>Small</td><td>-</td><td>Small, Very Large</td><td>2</td><td>1</td></tr>
         <tr><td>Small</td><td>-</td><td>Large, Very Large</td><td>0</td><td>3</td></tr>
         <tr><td>Small</td><td>Long</td><td>-</td><td>0</td><td>1</td></tr>
         <tr><td>Small</td><td>Long</td><td>Small</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>Long</td><td>Large</td><td>0</td><td>1</td></tr>
         <tr><td>Small</td><td>Long</td><td>Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>Long</td><td>Small, Large</td><td>0</td><td>4</td></tr>
         <tr><td>Small</td><td>Long</td><td>Small, Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>Long</td><td>Large, Very Large</td><td>0</td><td>1</td></tr>
         <tr><td>Small</td><td>Short</td><td>-</td><td>2</td><td>3</td></tr>
         <tr><td>Small</td><td>Short</td><td>Small</td><td>2</td><td>1</td></tr>
         <tr><td>Small</td><td>Short</td><td>Large</td><td>0</td><td>3</td></tr>
         <tr><td>Small</td><td>Short</td><td>Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>Short</td><td>Small, Large</td><td>2</td><td>4</td></tr>
         <tr><td>Small</td><td>Short</td><td>Small, Very Large</td><td>2</td><td>1</td></tr>
         <tr><td>Small</td><td>Short</td><td>Large, Very Large</td><td>0</td><td>3</td></tr>
      </tbody>
    </table>

    <p>
      Now the hypothesis space has be computed, the sizes of the sets
      $A, B, V$ can be derived.  
    </p>

    <p>
      $A$, the set of all complete hypotheses will contain all
      hypothesis the cover all positive examples; they will has a
      positive count of 4. Counting the hypotheses above there are only
      2 hypotheses covering all positive examples.
    </p>

    <p>
      $B$, the set of all consistent hypotheses that cover 1 or more
      positive examples can be computed by counting all hypotheses that
      cover no negative examples (the definition of consistent) and then
      counting the number of selected hypotheses that have 1 or more
      positive examples. There are 16 of these.
    </p>
    
  hint: |
    Enumerate the possible conjunctive hypotheses and count how many
    positive and negative examples are covered
  comments: |
    Tests the student's knowledge of conjunctive concepts, how to form
    the hypothesis space of conjunctive concepts, the definitions of
    complete and consistent hypotheses and version space.
    <br>
    Complexity 3 because it involves enumerating the hypothesis space
    then building some sets of hypotheses based on definitional knolwedge.
