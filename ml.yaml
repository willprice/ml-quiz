title: Machine Learning Quiz
url: "http://willprice.github.io/ml-quiz/"
candidate_number:
  - "64496"
  - "54321"
"1":
  images: []
  problem_type: training
  difficulty: "3"
  reference: "1.2"
  source: "http://www.csd.uwo.ca/courses/CS9840a/Lecture2_knn.pdf"
  question: |
    Given the training data

    $$\mathbf{x} =
      \begin{pmatrix}
        3 & 3 \\
        1 & 0 \\
        1 & 2 \\
        2 & 3 \\
        2 & 1 \\
      \end{pmatrix}$$

    with the labels:

    $$\mathbf{y} =
    \begin{pmatrix}
        0 \\
        1 \\
        0 \\
        0 \\
        1 \\
      \end{pmatrix}$$

    where an instance belonging to the $\oplus$ class is represented by
    $1$ in $\mathbf{y}$, and an instance belonging to the $\ominus$
    class by $0$.

    Classify the test instance $(0, 1)$ using the 3 nearest neighbours
    classifier with Euclidean distances by calculation
  answer_type: single
  answers:
    - correctness: "+"
      explanation: "The nearest points to $(0, 1)$ are $(1, 0)$, $(1, 2)$ and $(2, 1)$"
      answer: "$\\oplus$"
    - correctness: "-"
      answer: "$\\ominus$"
  workings: |
    We need to find the distance between our test point and every point
    in the training data set.

    First we find the distance between the test point and training
    points in each dimension.

    $$\mathbf{x} - (0, 1) =
    \begin{pmatrix}
      3 & 2 \\
      1 & -1 \\
      1 & 1 \\
      2 & 2 \\
      2 & 0 \\
    \end{pmatrix}$$

    We can then apply our distance measure to work out the distances.

    $$\mathbf{D(x, (0, 1))} =
    \begin{pmatrix}
      \sqrt{(3^2 + 2^2)} \\
      \sqrt{(1^2 + {(-1)}^2)} \\
      \sqrt{(1^2 + 1^2)} \\
      \sqrt{(2^2 + 2^2)} \\
      \sqrt{(2^2 + 0^2)} \\
    \end{pmatrix}
    =
    \begin{pmatrix}
      3.6 \\
      1.4 \\
      1.4 \\
      2.8 \\
      2 \\
    \end{pmatrix}$$

    The 2nd, 3rd and 5th entries are the closest to the test point,
    looking this up in our training data vector $\mathbf{x}$ we see they
    correspond to the following points, $(1, 0)$, $(1, 2)$ and $(2, 1)$,
    which each have the classes $\oplus$, $\ominus$, $\oplus$ respectively.
    We take the majority class as the prediction for our test point, $\oplus$.
  hint: |
    First plot the data then see whether you can visually identify the
    points that are closest.
  comments: |
    Tests the student's knowledge of training and using a KNN model.
    <br>

    Since the student has to both train and test using classifier, a
    fair amount of calculation, this question scores a 3 in difficulty.


"2":
  images: []
  problem_type: problem-solving
  difficulty: "3"
  reference: "2.1"
  source: "https://en.wikipedia.org/wiki/Precision_and_recall"
  question: |
    Fill in the contingency table.
    <table>
      <tr>
        <th>Measure</th>
        <th>Value</th>
      </tr>
      <tr>
        <td>$\sum_{x \in Te}\left[ I(\hat{c}(x) = \oplus \right] = 45$</td>
        <td>45</td>
      </tr>
      <tr>
        <td>$\sum_{x \in Te}\left[ I(\hat{c}(x) = \ominus \right] = 20$</td>
        <td>20</td>
      </tr>
      <tr>
        <td>Precision</td>
        <td>$\frac{8}{9}$</td>
      </tr>
      <tr>
        <td>False positive rate</td>
        <td>$\frac{1}{4}$</td>
      </tr>
    </table>
  answer_type: cloze_answer
  answers:
    answer:
      - "40 | 5  | 45"
      - "------------"
      - "5  | 15 | 20"
      - "------------"
      - "45 | 20 | 65"
    explanation: See the workings for derivation
  workings: |
    Recall the following defintions
    $$\text{Precision} = \frac{TP}{TP + FP}$$
    $$\text{False positive rate} = \frac{FP}{Neg} = \frac{FP}{FP + TN}$$
    <br>

    <table>
      <thead>
        <tr>
          <th></th>
          <th>Predicted +</th>
          <th>Predicted -</th>
          <th>Marginals</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <td>Actual +</td>
          <td>$TP$</td>
          <td>$FN$</td>
          <td>$TP + FN$</td>
        </tr>
        <tr>
          <td>Actual -</td>
          <td>$FP$</td>
          <td>$TN$</td>
          <td>$FP + TN$</td>
        </tr>
        <tr>
          <td>Marginals</td>
          <td>$TP + FP$</td>
          <td>$FN + TN$</td>
          <td>$|Te| = TP + FP + FN + TN$</td>
        </tr>
      </tbody>
    </table>

    We need to derive set of simultaneous equations describing the
    information given.

    $$
    \mathrm{precision} = \frac{8}{9} \Leftrightarrow \frac{TP}{TP + FP}
      = \frac{8}{9} \Leftrightarrow TP = 8 \cdot FP$$
    $$\mathrm{fpr} = \frac{1}{4} \Leftrightarrow \frac{FP}{FP + TN} = \frac{1}{4}
      \Leftrightarrow TN = 3 \cdot FP$$

    Using the above with the sums of predicted classes we have the
    following four equations, numbered 1 to 4 from top to bottom.

    $$
    \begin{align}
    TP + FP &= 45 \\
    TN + FN &= 20 \\
    TP &= 8 \cdot FP \\
    TN &= 3 \cdot FP \\
    \end{align}
    $$

    Substituting eq. 3 into eq. 1 yields $FP = 5$ and hence $TP = 40$,
    we then solve for $TN$ using eq. 4, $TN = 15$, finally solving for
    $FN$ using eq. 2, $FN = 5$

  hint: |
    What are the definitions of *precision* and *false positive rate*?
    Try to derive them in terms of the individual elements of the
    contingency table.
  comments: |
    Tests the student's knowledge of contingency tables: what does each
    cell mean, how are the marginals calculated. It also tests the
    definitions of precision and false positive rate in terms of true
    positives, true negatives, and false positives.
    <br>
    The additional twist of solving with simultaneous equations bumps
    the complexity up from a 2 to 3.
"3":
  images: []
  problem_type: problem-solving
  difficulty: "3"
  reference: "3.3"
  source: "URL"
  question:
    - |
      <p>
      We're trying to find out the characteristics of rodents, so a
      variety of animals were sampled and certain features about them
      were recorded. We'd like to come up with some conjunctive concepts
      to determine whether a certain animal is a rodent based on it's
      features.
      </p>

      <p>
      The examples below cover the full range of feature values, (i.e.
      there is no 'Very long' value for ear size)
      </p>

      <table>
      <thead>
        <tr>
          <th>Eye size</th>
          <th>Tail length</th>
          <th>Ear size</th>
          <th>Class</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <!-- mouse -->
          <td>Large</td>
          <td>Long</td>
          <td>Very Large</td>
          <td>+</td>
        </tr>
        <tr>
          <td>Small</td>
          <td>Short</td>
          <td>Large</td>
          <td>-</td>
        </tr>
        <tr>
          <!-- vole -->
          <td>Small</td>
          <td>Short</td>
          <td>Small</td>
          <td>+</td>
        </tr>
        <tr>
          <!-- shrew -->
          <td>Small</td>
          <td>Short</td>
          <td>Small</td>
          <td>+</td>
        </tr>

        <tr>
          <td>Small</td>
          <td>Long</td>
          <td>Large</td>
          <td>-</td>
        </tr>
        <tr>
          <!-- possum -->
          <td>Large</td>
          <td>Long</td>
          <td>Small</td>
          <td>+</td>
        </tr>
        <tr>
          <td>Small</td>
          <td>Short</td>
          <td>Large</td>
          <td>-</td>
        </tr>
        <tr>
          <td>Small</td>
          <td>Short</td>
          <td>Small</td>
          <td>-</td>
        </tr>
      </tbody>
      </table>

      <p>
      The following sets of hypotheses are defined
      <ul>
       <li>$A$, the set of complete hypotheses</li>
       <li>$B$, the set of consistent hypotheses, covering at least one positive example</li>
       <li>$V$, the version space</li>
      </ul>
    - |
      <br />
      What is $|A|$
    - 1
    - |
      <br />
      What is $|B|$
    - 2
    - |
      <br />
      What is $|V|$
    - 3
    - </p>

  answer_type: blank_answer
  answers:
    - correctness: 1
      answer: "2"
      explanation: The number of hypotheses that cover all the positive examples
    - correctness: 2
      answer: "16"
      explanation: |
        The number of hypotheses that cover none of the negative
        examples and at least one of the positive examples
    - correctness: 3
      answer: "0"
      explanation: There are no complete AND consistent hypotheses
  workings: |
    <p>
    The hypothesis space of all conjunctive concepts needs to be
    computed, then for each conjunctive concept we have to count the
    number of positive and negative examples, from that we can derive
    the size of each of the sets of hypotheses.
    </p>

    <p>
    First it's sensible to reoder the table to separate positive and
    negative examples to help ease counting.
    </p>

    <table>
    <thead>
      <tr>
        <th>Eye size</th>
        <th>Tail length</th>
        <th>Ear size</th>
        <th>Class</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <!-- mouse -->
        <td>Large</td>
        <td>Long</td>
        <td>Very Large</td>
        <td>+</td>
      </tr>
      <tr>
        <!-- vole -->
        <td>Small</td>
        <td>Short</td>
        <td>Small</td>
        <td>+</td>
      </tr>
      <tr>
        <!-- shrew -->
        <td>Small</td>
        <td>Short</td>
        <td>Small</td>
        <td>+</td>
      </tr>
      <tr>
        <!-- possum -->
        <td>Large</td>
        <td>Long</td>
        <td>Small</td>
        <td>+</td>
      </tr>

      <tr>
        <td>Small</td>
        <td>Short</td>
        <td>Large</td>
        <td>-</td>
      </tr>
      <tr>
        <td>Small</td>
        <td>Long</td>
        <td>Large</td>
        <td>-</td>
      </tr>
      <tr>
        <td>Small</td>
        <td>Short</td>
        <td>Large</td>
        <td>-</td>
      </tr>
      <tr>
        <td>Small</td>
        <td>Short</td>
        <td>Small</td>
        <td>-</td>
      </tr>
    </tbody>
    </table>

    <p>
      Each feature can take on one of several values, each conjunctive
      literal will use an internal disjunction of these values. The
      possible internal disjunctions are calculated as they are
      necessary to build the hypothesis space.

      We denote the internal disjuction of all possible values with '-'
      as this functions as a wildcard, a "don't care" value, since the
      feature can take on any value and we will still match the instance.
    </p>


    <p>
     Every possible hypothesis is computed by the combination over all
     the possible interal disjunctions of each feature, this forms
     something similar to a truth table and can be constructed in a
     similar fashion.
    </p>

    <table>
      <thead>
        <tr><th>Eye size</th><th>Tail length</th><th>Ear size</th><th>#Pos</th><th>#Neg</th></tr>
      </thead>
      <tbody>
        <tr><td>-</td><td>-</td><td>-</td><td>4</td><td>4</td></tr>
         <tr><td>-</td><td>-</td><td>Small</td><td>3</td><td>1</td></tr>
         <tr><td>-</td><td>-</td><td>Large</td><td>0</td><td>3</td></tr>
         <tr><td>-</td><td>-</td><td>Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>-</td><td>-</td><td>Small, Large</td><td>3</td><td>4</td></tr>
         <tr><td>-</td><td>-</td><td>Small, Very Large</td><td>4</td><td>1</td></tr>
         <tr><td>-</td><td>-</td><td>Large, Very Large</td><td>1</td><td>3</td></tr>
         <tr><td>-</td><td>Long</td><td>-</td><td>2</td><td>1</td></tr>
         <tr><td>-</td><td>Long</td><td>Small</td><td>1</td><td>0</td></tr>
         <tr><td>-</td><td>Long</td><td>Large</td><td>0</td><td>1</td></tr>
         <tr><td>-</td><td>Long</td><td>Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>-</td><td>Long</td><td>Small, Large</td><td>1</td><td>1</td></tr>
         <tr><td>-</td><td>Long</td><td>Small, Very Large</td><td>2</td><td>0</td></tr>
         <tr><td>-</td><td>Long</td><td>Large, Very Large</td><td>1</td><td>1</td></tr>
         <tr><td>-</td><td>Short</td><td>-</td><td>2</td><td>3</td></tr>
         <tr><td>-</td><td>Short</td><td>Small</td><td>2</td><td>1</td></tr>
         <tr><td>-</td><td>Short</td><td>Large</td><td>0</td><td>2</td></tr>
         <tr><td>-</td><td>Short</td><td>Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>-</td><td>Short</td><td>Small, Large</td><td>2</td><td>3</td></tr>
         <tr><td>-</td><td>Short</td><td>Small, Very Large</td><td>2</td><td>1</td></tr>
         <tr><td>-</td><td>Short</td><td>Large, Very Large</td><td>0</td><td>2</td></tr>
         <tr><td>Large</td><td>-</td><td>-</td><td>2</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Small</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Small, Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Small, Very Large</td><td>2</td><td>0</td></tr>
         <tr><td>Large</td><td>-</td><td>Large, Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>-</td><td>2</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Small</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Small, Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Small, Very Large</td><td>2</td><td>0</td></tr>
         <tr><td>Large</td><td>Long</td><td>Large, Very Large</td><td>1</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>-</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Small</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Small, Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Small, Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Large</td><td>Short</td><td>Large, Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>-</td><td>-</td><td>2</td><td>4</td></tr>
         <tr><td>Small</td><td>-</td><td>Small</td><td>2</td><td>1</td></tr>
         <tr><td>Small</td><td>-</td><td>Large</td><td>0</td><td>3</td></tr>
         <tr><td>Small</td><td>-</td><td>Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>-</td><td>Small, Large</td><td>2</td><td>4</td></tr>
         <tr><td>Small</td><td>-</td><td>Small, Very Large</td><td>2</td><td>1</td></tr>
         <tr><td>Small</td><td>-</td><td>Large, Very Large</td><td>0</td><td>3</td></tr>
         <tr><td>Small</td><td>Long</td><td>-</td><td>0</td><td>1</td></tr>
         <tr><td>Small</td><td>Long</td><td>Small</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>Long</td><td>Large</td><td>0</td><td>1</td></tr>
         <tr><td>Small</td><td>Long</td><td>Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>Long</td><td>Small, Large</td><td>0</td><td>4</td></tr>
         <tr><td>Small</td><td>Long</td><td>Small, Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>Long</td><td>Large, Very Large</td><td>0</td><td>1</td></tr>
         <tr><td>Small</td><td>Short</td><td>-</td><td>2</td><td>3</td></tr>
         <tr><td>Small</td><td>Short</td><td>Small</td><td>2</td><td>1</td></tr>
         <tr><td>Small</td><td>Short</td><td>Large</td><td>0</td><td>3</td></tr>
         <tr><td>Small</td><td>Short</td><td>Very Large</td><td>0</td><td>0</td></tr>
         <tr><td>Small</td><td>Short</td><td>Small, Large</td><td>2</td><td>4</td></tr>
         <tr><td>Small</td><td>Short</td><td>Small, Very Large</td><td>2</td><td>1</td></tr>
         <tr><td>Small</td><td>Short</td><td>Large, Very Large</td><td>0</td><td>3</td></tr>
      </tbody>
    </table>

    <p>
      Now the hypothesis space has be computed, the sizes of the sets
      $A, B, V$ can be derived.
    </p>

    <p>
      $A$, the set of all complete hypotheses will contain all
      hypothesis the cover all positive examples; they will has a
      positive count of 4. Counting the hypotheses above there are only
      2 hypotheses covering all positive examples.
    </p>

    <p>
      $B$, the set of all consistent hypotheses that cover 1 or more
      positive examples can be computed by counting all hypotheses that
      cover no negative examples (the definition of consistent) and then
      counting the number of selected hypotheses that have 1 or more
      positive examples. There are 16 of these.
    </p>

  hint: |
    Enumerate the possible conjunctive hypotheses and count how many
    positive and negative examples are covered
  comments: |
    Tests the student's knowledge of conjunctive concepts, how to form
    the hypothesis space of conjunctive concepts, the definitions of
    complete and consistent hypotheses and version space.
    <br>
    Complexity 3 because it involves enumerating the hypothesis space
    then building some sets of hypotheses based on definitional knolwedge.

"4":
  images: 
    - url: img/nn-backprop-question-network.png
      caption: "Fig 1: Example neural network"
    - url: img/nn-backprop-question-circuit.png
      caption: |-
        Fig 2: Neural network from Fig 1 drawn as a circuit diagram,
        showing how to calculate intermediate values in forward propagation 
    - url: img/nn-backprop-question-circuit-answer-abstract.png
      caption: |-
        Fig 3: Neural network from Fig 1 drawn as a circuit diagram,
        showing how to calculate intermediate values in back propagation
  problem_type: training
  difficulty: "5"
  reference: "0"
  source: "https://cs231n.github.io/optimization-1/"
  question:
    - |
      <p>
      Neural networks (NNs) are a versatile of model used to great success
      in many different domains. A neural network is a directed graph of
      nodes and edges. Nodes (neurons), either represent inputs or compute
      some function of their inputs. Edges represent connections between
      adjacent neurons, they have a weight associated with them. Most
      neurons compute the weighted sum of their inputs followed by a
      non-linear function application (such as the $\sigma$, or
      $\tanh$ function.
      </p>

      <p>
      NNs can be trained using a technique called
      <emph>backpropagation</emph>. The main idea behind backpropagation
      is the rate of change of the loss of an output neuron can be
      calculated with respect to the weights of the network through
      repeated application of the chain rule. The weights of the network
      can then be updated using gradient descent to reduce the loss.
      </p>


      <p>
      One might expect backpropagation to be very complicated as the
      number of neurons between an input and output neuron can be large,
      leading to a very complex gradient function, however the chain rule
      means we can decompose this into a series of simple calculations
      evaluating the derivative of the output of each neuron with respect
      to its input, these can then be combined to through successive
      multiplications to calculate the change in loss with respect to a
      specific weight.
      </p>

      <p>
      A simple network is presented in Fig 1 composed of two
      input neurons and one output neuron. The output neuron computes the
      weighted sum of its inputs and applies the sigmoid function to
      produce its output.
      </p>

      <p>
      We will investigate how neural networks function by forward
      propagating (computing the output of the last layer of the network
      from a set of inputs), then backpropagating through the network to
      perform weight updates.
      </p>

      <p>
      Mathematical functions can be thought of as gates in a circuit that
      take inputs and produce an ouput, we will redraw the network in a
      circuit style breaking down the network into gates that we use to
      help perform back propagation.
      </p>

      <p>
      The network in Fig 1 has been redrawn as a circuit in Fig 2. Forward
      propagation is accomplished by applying the function specified in
      the neuron to its inputs, the calculations necessary to compute the
      output of each neuron are specified on the arrow between adjacent
      neurons. For the initial weights values $(w_1, w_2, w_3) = (2, -3,
      -3)$, and training example $(x_1, x_2) = (-1, -2)$, compute the
      top labelled values (those computed during forward propagation) in
      Fig 3.
      </p>

      <ul>
      <li>$a_1$
    - 1
    - |
      </li>
      <li>$a_2$
    - 2
    - |
      </li>
      <li>$b$
    - 3
    - |
      </li>
      <li>$c$
    - 4
    - |
      </li>
      <li>$\hat{y}$
    - 5
    - |
      </li>
      </ul>

    - |
      <p>
      Having got a feel for how forward propagation works, we'll now
      show you how to backpropagate. When training the network we need
      some way of quantifying the difference in predicted output to the
      ground truth. Loss functions are used to compute this difference,
      some common examples of loss functions include cross-entropy loss; and
      euclidean loss, the scaled sum of the squared error of each output
      neuron. Our example only has a single output neuron, so for the
      sake of simplicity we'll define loss as the difference between the
      output and the expected output.

      Our goal is to find
      $$\nabla_{w} L = \left[
        \frac{\partial L}{\partial w_1},
        \frac{\partial L}{\partial w_2},
        \ldots,
        \frac{\partial L}{\partial w_k}
      \right]$$

      Each term in the vector defines how the loss changes as you
      change a specific weight. $L$ represents a surface in a $k$
      dimensional space (if we hold $x$ constant), $\nabla_{w} L$ is the
      gradient at a specific point in that space defined by the current
      weights of the network. Since we want to reduce the loss and make
      our network more accurate, we can use gradient descent to traverse
      the surface to a hollow, this is not necessarily a global minima,
      but in practice gradient descent works well and produces networks
      that successfully function in their task.
      </p>

      <p>
      Let's look at a component of $\nabla_w L$, and figure out how to
      derive it.
      $$\frac{\partial L}{\partial w_1}$$
      Recall this represents how the loss of the network changes as we
      change weight $w_1$. We can decompose this into the product of
      multiple partial derivatives, each partial derivative representing
      how the output of a neuron changes with respect to its input, until
      we reach the beginning of the network.
      </p>

      <p>
      The input to each neuron has been labelled with a variable name on
      Fig 2, the partial derivatives of the loss with respect to the weights are:
      $$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial b} 
                                          \frac{\partial b}{\partial a_1}
                                          \frac{\partial a_1}{\partial w_1}$$
      $$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial b} 
                                          \frac{\partial b}{\partial a_2}
                                          \frac{\partial a_2}{\partial w_2}$$
      If we compute the analytical derivative of the output of each
      neuron with respect to its input, we can construct an algorithm to
      work backwards through the network to compute the gradient of the
      loss with respect to the weights.
      </p>

      <p>
      Compute the following derivatives:
        $$\frac{d}{dx}(x + y)$$
        $$\frac{d}{dx}(x \cdot y)$$
        $$\frac{d}{dx}(\sigma(y))$$
      Where
        $$\sigma(x) = \frac{1}{1 + e^x}$$
      </p>

      <p>
      Finally we can backpropagate and find out how changing the weights
      affects the loss, starting from the output gate we can trace our
      way back to an input weight by accumulating the derivative product
      (i.e. incrementally build up the full partial derivative via the
      chain rule). As we traverse a gate, we compute the partial
      derivative and multiply the accumulator by the derivative until
      reaching the input weight.
      </p>

      <p>
      Following the above process, backpropagate the circuit in Fig 3.
      </p>

      What are the following partial derivatives?

      <ul>
      <li>$\frac{\partial L}{\partial w_1}$ = 
    - 6
    - |
      </li>
      <li>$\frac{\partial L}{\partial w_2}$ = 
    - 7
    - |
      </li>
      <li>$\frac{\partial L}{\partial w_3}$ = 
    - 8
    - |
      </li>
      </ul>


  answer_type: blank_answer
  answers:
    - correctness: 1
      answer: "-2"
      explanation: See the workings for derivation
    - correctness: 2
      answer: "6"
      explanation: See the workings for derivation
    - correctness: 3
      answer: "4"
      explanation: See the workings for derivation
    - correctness: 4
      answer: "1"
      explanation: See the workings for derivation
    - correctness: 5
      answer: "0.73"
      explanation: See the workings for derivation
    - correctness: 6
      answer: "-0.20"
      explanation: See the workings for derivation
    - correctness: 7
      answer: "-0.40"
      explanation: See the workings for derivation
    - correctness: 8
      answer: "0.20"
      explanation: See the workings for derivation
  workings: |-
    <p>
    Solving for the intermediate values in forward propagation:
    $$\begin{align}
      a_1 &= x_1w_1 = -1 \cdot 2 = -2 \\
      a_2 &= x_2w_2 = -2 \cdot -3 = 6 \\
      b &= x_1w_1 + x_2w_2 = -2 + 6 = 4 \\
      c &= (x_1w_1 + x_2w_2) + w_3= 4 + (-3) = 1 \\
      \hat{y} &= \sigma(x_1w_1 + x_2w_2 + w_3)= \sigma(1) = \frac{1}{1 + e^{-1}} = 0.731
    \end{align}$$
    </p>

    <p>
    Derivation of $\frac{d}{dx}(x + y)$
    $$\begin{align}
        \frac{d}{dx}(x + y) &= 1
      \end{align}$$
    </p>

    <p>
    Derivation of $\frac{d}{dx}(xy)$
    $$\begin{align}
        \frac{d}{dx}(xy) &= y
      \end{align}$$
    </p>

    <p>
    Derivation of $\frac{d}{dx}\sigma(x)$
    $$\begin{align}
        \frac{d}{dx}\sigma(x) &= \frac{d}{dx}\left(\frac{1}{1 + e^{-x}}\right) \\
                              &= \frac{d}{du}\left(\frac{1}{u}\right) \cdot \frac{d}{dx}\left(1 + e^{-x}\right) \\
                              &= -(1 + e^{-x})^{-2} \cdot e^{-x} \cdot (-1) \\
                              &= (1 + e^{-x})^{-2} \cdot e^{-x} \\
                              &= \frac{1}{(1 + e^{-x})^2} \cdot e^{-x} \\
                              &= \frac{e^{-x}}{1 + e^{-x}} \cdot \frac{1}{1 + e^{-x}} \\
                              &= \frac{1 + e^{-x} - 1}{1 + e^{-x}} \cdot \frac{1}{1 + e^{-x}} \\
                              &= \left(\frac{1 + e^{-x}}{1 + e^{-x}} - \frac{1}{1 + e^{-x}}\right) \cdot \frac{1}{1 + e^{-x}} \\
                              &= \left(1 - \frac{1}{1 + e^{-x}}\right) \cdot \frac{1}{1 + e^{-x}} \\
                              &= \sigma(x) \left( 1 - \sigma(x) \right)
      \end{align}$$
      </p>

      <p>
      Solving for intermediate values in back propagation using the
      results above yields the following analytical solutions

      $$\frac{\partial L}{\partial c} = \frac{\partial \sigma(c)}{\partial c} = \sigma(c)(1 - \sigma(c))$$
      $$\frac{\partial c}{\partial b} = \frac{\partial (b + w_3)}{\partial b} = 1$$
      $$\frac{\partial c}{\partial w_3} = \frac{\partial (b + w_3)}{\partial w_3} = 1$$
      $$\frac{\partial b}{\partial a_1} = \frac{\partial (a_1 + a_2)}{\partial a_1} = 1$$
      $$\frac{\partial b}{\partial a_2} = \frac{\partial (a_1 + a_2)}{\partial a_2} = 1$$
      $$\frac{\partial a_1}{\partial w_1} = \frac{\partial (x_1w_1)}{\partial w_1} = x_1$$
      $$\frac{\partial a_2}{\partial w_2} = \frac{\partial (x_2w_2)}{\partial w_2} = x_2$$

      Substituting the values calculated in forward propagation:

      $$\frac{\partial L}{\partial c} = 0.731(1 - 0.731) = 0.200$$
      $$\frac{\partial c}{\partial b} = 1$$
      $$\frac{\partial c}{\partial w_3} = 1$$
      $$\frac{\partial b}{\partial a_1} = 1$$
      $$\frac{\partial b}{\partial a_2} = 1$$
      $$\frac{\partial a_1}{\partial w_1} = x_1 = -1$$
      $$\frac{\partial a_2}{\partial w_2} = x_2 = -2$$

      Using the chain rule we can find the desired partial derivatives:
      $$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial c}
                                          \frac{\partial c}{\partial b}
                                          \frac{\partial b}{\partial a_1}
                                          \frac{\partial a_1}{\partial w_1}
                                        = 0.200 \cdot 1 \cdot 1 \cdot -1 = -0.20
      $$
      $$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial c}
                                          \frac{\partial c}{\partial b}
                                          \frac{\partial b}{\partial a_2}
                                          \frac{\partial a_2}{\partial w_2}
                                        = 0.200 \cdot 1 \cdot 1 \cdot -2 = -0.40
      $$
      $$\frac{\partial L}{\partial w_3} = \frac{\partial L}{\partial c}
                                          \frac{\partial c}{\partial w_3}
                                        = 0.200 \cdot 1 = 0.20
      $$
      </p>
  hint: |
    Blah

  comments: |
    Tests the student's knowledge of neural networks and the circuit
    interpretation of backpropagation.

    <br />
    Complexity 5 because it is out the scope of the book and requires
    both calculation and insight to derive the derivative of
    $$\sigma(x)$$ in terms of itself
