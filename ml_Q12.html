<!doctype html>
<!--[if !IE]>
<html class="no-js non-ie" lang="en-US">

<![endif]-->
<!--[if IE 7 ]>
<html class="no-js ie7" lang="en-US">

<![endif]-->
<!--[if IE 8 ]>
<html class="no-js ie8" lang="en-US">

<![endif]-->
<!--[if IE 9 ]>
<html class="no-js ie9" lang="en-US">

<![endif]-->
<!--[if gt IE 9]>
<!-->
<html class="no-js" lang="en-US">
  <!--
<![endif]-->
  <head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
      Machine Learning &#124; Quizzes
    </title>
    <link rel="profile" href="http://gmpg.org/xfn/11"/>
    <link rel="pingback" href="http://python.otuama.net/xmlrpc.php"/>
    <link rel="alternate" type="application/rss+xml" title="Machine Learning &raquo; Feed" href="http://python.otuama.net/feed/"/>
    <link rel="alternate" type="application/rss+xml" title="Machine Learning &raquo; Comments Feed" href="http://python.otuama.net/comments/feed/"/>
    <link rel='stylesheet' id='wpProQuiz_front_style-css' href='resources/css/wpProQuiz_front.min.css?ver=0.28' type='text/css' media='all'/>
    <link rel='stylesheet' id='responsive-style-css' href='resources/css/responsive_style.css?ver=1.9.3.4' type='text/css' media='all'/>
    <link rel='stylesheet' id='responsive-media-queries-css' href='resources/css/responsive_core_style.css?ver=1.9.3.4' type='text/css' media='all'/>
    <script type='text/javascript' src='resources/js/wp_jquery.js?ver=1.10.2'>
    </script>
    <script type='text/javascript' src='resources/js/wp_jquery-migrate.min.js?ver=1.2.1'>
    </script>
    <script type='text/javascript' src='resources/js/responsive_core_responsive-modernizr.js?ver=2.6.1'>
    </script>
    <link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://python.otuama.net/xmlrpc.php?rsd"/>
    <link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://python.otuama.net/wp-includes/wlwmanifest.xml"/>
    <meta name="generator" content="WordPress 3.6.1"/>
    <!-- We need this for debugging -->
    <!-- Responsive 1.9.3.8 -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$','$']],
          displayMath: [ ['$$','$$'] ]
        }
      });
    </script>
    <script
    type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

    </script>
  </head>
  <body class="home blog">
    <div id="container" class="hfeed">
      <div id="header">
    <div id="logo">

      </div>
      <!-- end of #logo -->
  </div>
  <!-- end of #header -->
  <div id="wrapper" class="clearfix">
    <div id="content-blog" class="grid col-940">
      <!-- Blog page title -->
      <h1>
      </h1>
      <div id="post-10" class="post-10 post type-post status-publish format-standard hentry category-distance-models category-quiz">
        <h2 class="entry-title post-title">
          <a href="#" rel="bookmark">
            Machine Learning Quiz
          </a>
        </h2>


        <!-- end of .post-meta -->
        <div class="post-entry">
          <div class="wpProQuiz_content" id="wpProQuiz_2">
            <h2>Machine Learning Quiz</h2>




          <div style="display: none;" class="wpProQuiz_results">
            <h4 class="wpProQuiz_header">
              Results
            </h4>
            <p class="wpProQuiz_time_limit_expired" style="display: none;">
              Time has elapsed
            </p>
            <div class="wpProQuiz_catOverview" style="display:none;">
              <h4>
                Categories
              </h4>
              <div style="margin-top: 10px;">
                <ol>
                  <li data-category_id="0">
                    <span class="wpProQuiz_catName">
                      Not categorized
                    </span>
                    <span class="wpProQuiz_catPercent">
                    </span>
                  </li>
                  <li data-category_id="4">
                    <span class="wpProQuiz_catName">
                      Distance
                    </span>
                    <span class="wpProQuiz_catPercent">
                    </span>
                  </li>
                </ol>
              </div>
            </div>
            <div>
              <ul class="wpProQuiz_resultsList">
                <li style="display: none;">
                  <div>
                  </div>
                </li>
              </ul>
            </div>
            <div style="margin: 10px 0px;">
              <input class="wpProQuiz_button" type="button" name="restartQuiz" value="Restart quiz">
              <input class="wpProQuiz_button" type="button" name="reShowQuestion" value="View questions">
            </div>
          </div>

          <div class="wpProQuiz_quizAnker" style="display: none;">
          </div>
          <div style="display: none;" class="wpProQuiz_quiz">
            <ol class="wpProQuiz_list">

<!-- START OF QUESTION 12 -->

<li class="wpProQuiz_listItem" style="display: none;">
  <h5 style="display: inline-block;" class="wpProQuiz_header"><span>12</span>. Question</h5>
  <div style="font-weight: bold; padding-top: 5px;">Category: 5.1: Tree models : Decision trees</div>
  <div style="font-weight: bold; padding-top: 5px;">Category: medium (3)</div>

  <!-- QUESTION (SINGLE ANSWER) -->
  <div class="wpProQuiz_question" style="margin: 10px 0px 0px 0px;">

    <!-- QUESTION -->
    <div class="wpProQuiz_question_text"><p><div style="border-style:solid; border-width:5px; border-color:#ff0000 #0000ff;">workings:<br><div style="display: none">
$$\(
\DeclareMathOperator{\imp}{Imp}
\)$$
</div>
<p>
GrowTree(D,F) is a divide-and-conquer algorithm that in our case given the
D instance space containing 10 samples and the F set of features will return
a feature tree T with labbelled leaves, assuring that each labeled leaf is
is homogeneous and each split is given by BestSplit-Class(D,F) algorithm.
</p>

<pre>
<code>
The GrowTree(D,F) is as follows:

Input : data D; features D.
Output: feature tree T with labbelled leaves.
if Homogeneous(D) then return Label(D);
S<-BestSplit(D,F);
split D into subsets D(i) according to the literals in S;
for each i do
  if D(i) != {} then
    T(i) <- GrowTree(D(i),F)
    else
      T(i) is a leaf labelled with Label(D)
end
return a tree whose root is labelled with S and whose children are T(i)
</code>
</pre>


<p>
The BestSplit-class algorithm chooses as the best feature to spilt on, the one
that achieves the maximum reduction in impurity and in the case of
using entropy as the impurity metric, the maximum information gain.
The gain in impurity from a parent node $D$ to a set of children
$\{D_1, ..., D_l\}$ is defined as $\imp(D) - \imp(\{D_1,...,D_l\})$.
</p>

<p>
When choosing the best feature to split, we can simply choose the
feature that produces the minimum weighted impurity since, the
impurity of the parent is constant when testing out all the different
feature splits.
</p>

<p>
Splitting the data set based on the feature on the left results in the
child leaf counts on the right

\begin{align}
Length &= 3           \rightarrow [2+, 0-]\\
Length &= 4           \rightarrow [1+, 3-]\\
Length &= 5           \rightarrow [2+, 2-]\\
Gills  &= \text{yes}  \rightarrow [0+, 4-]\\
Gills  &= \text{no}   \rightarrow [5+, 1-]\\
Beak   &= \text{yes}  \rightarrow [5+, 3-]\\
Beak   &= \text{no}   \rightarrow [0+, 2-]\\
Teeth  &= \text{many} \rightarrow [3+, 4-]\\
Teeth  &= \text{few}  \rightarrow [2+, 1-]\\
\end{align}
</p>

<p>
Using these counts and entropy as our purity measure we can determine
which feature produces the best split by finding the feature that
produced the minimum weight sum of impurities of the child leaves.
</p>

<p>
We will abbreviate features and values for brevity, length becomes
$L$, gills becomes $G$, beak becomes $B$, and so on.

<p>
<b>Split on Length</b><br />
For $L = 3$ we only have positive instances meaning the leaf will
be pure so therefore we have 

$$\imp(D_{L= 3}) = 0$$

$$\imp(D_{L = 4}) = -(1/4) \log(1/4) - (3/4) \log(3/4) = 0.5 + 0.31 = 0.81$$

For $L = 5$, the leaf has the same number of each class so impurity as
it a maximum: $\imp(D_{L=5}) = 1$

Calculating the weighted sum of impurity:

$$\imp(\{D_{L = 3}, D_{L = 4}, D_{L = 5}\}) = 2/10 \cdot 0 + 4/10 \cdot 0.81 + 4/10 \cdot 1 = 0.72 $$
</p>


<p>
<b>Split on Gills</b><br />
For feature $G = Y$ we only have negative instances so we will have again
an entropy of 0.
<br />

$$\imp(D_{G=N}) = -(5/6) \log(5/6) - (1/6) \log(1/6)$$

Calculating the weighted sum of impurity:

$$\imp(\{D_{G=Y}, D_{G=N}\}) = 4/10 \cdot 0 + 6/10 \cdot(-(5/6)\log(5/6)-(1/6)\log(1/6)) = 0.39.  $$
</p>

<p>
<b>Split on Beak</b><br />
$$
\imp(\{D_{B=Y}, D_{B=N}\}) = 8/10 \cdot(-(5/8) \log(5/8) - (3/8) \log(3/8)) + 2/10 \cdot 0 = 0.76
$$
</p>

<p>
<b>Split on Teeth</b><br />
$$
\begin{align}
\imp(\{D_{T=F}, D_{T=M}\}) = & 7/10 \cdot(-(3/7) \log(3/7) - (4/7) \log(4/7)) + {} \\
                         & 3/10 \cdot(-(2/3) \log(2/3) - (1/3) \log(1/3)) \\
                         = &0.97
\end{align}
$$
</p>

<p>
As we already established that the BestSplit-class will choose the feature that
achieves the minimum impurity we can therefore say that 'Gills' will be the first
best feature to split on. The rest of the tree can be derived in a similar fashion.
</p>

<p>
Gills = no will be homogeneous over the negative class hence it will
be labeled that negative. In a similar vein, we will repeat the
labelling process on the Gills = yes branch until all the leafs
obtained until the leaves are all labelled. From top to bottom, the
decision tree will be in the end split in order on the following
features: gills, length, teeth.
</p>

<p>
However, that implies that we will in the end overfit our decision tree for the
data provided.To avoid this we can either stop our feature tree construction earlier,
process known as pre-prunning or we can otherwise reasses our splits in a bottom-up
manner provided a new set of data that was not part of our training procedure.
</p>

<p>
The latter will be realised using the PruneTree algorithm that takes as input
a labelled decision tree and a new labelled set of instances and outputs a
pruned tree T' as follows:
</p>

<pre>
<code>
PruneTree(T,D) is as follows:

Input : decision tree D; labelled data D;
Output: pruned tree T'.
for every internal node N of T, starting form the bottom do:
  T{N} <- subtree of T rooted at N;
  D{N} <- {x from D|x is covered by N}
  if accuracy of T{N} over D{N} is worse than majority class in D{N} then
    replace T{N} in T be a leaf labelled with the majority class in D{N}
    end
end
return pruned version of T
</code>
</pre>

<p>
Using the new data given teeth feature will classify a positive instances
as being negative in the case of teeth = few. However, if we were to take
the majority class of its parent we would be able to classify it correctly
having therefore a higher accuracy that will cause that subtree to be pruned.
Is it also revelant to say that now that all conditional branches of length
classify their instances as being positive we can prune that subtree as well,
keeping in the end only the split on the gills feature.
</p></div><hr><div style="border-style:solid; border-width:5px; border-color:#ff0000 #0000ff;">hint:<br><p>
Follow the given algorithms taking into consideration that finding the best
split will require finding the feature that achieves the minimum entropy/impurity
while at each added split the number of instances will reduce.
</p></div><hr><div style="border-style:solid; border-width:5px; border-color:#ff0000 #0000ff;">comment:<br><p>
Problem requires the understanding of basic algorithms used in the construction of
decision trees, while also introducing the problem of overfitting which can be solved
through prunning.
</p>

<p>
Complexity 3 because it requires extensive computations for determining the best
feature to split on and also the decision tree accuracy on the new data presented.
</p></div><hr><p>
Defining the following features:
$$
\begin{align}
Length &= [3,4,5]\\
Gills  &= [\text{yes}, \text{no}]\\
Beak   &= [\text{yes}, \text{no}]\\
Teeth  &= [\text{many}, \text{few}]\\
\end{align}
$$
and the following table of training samples:
</p>

<table>
   <tr>
      <td>Length</td>
      <td>Gills</td>
      <td>Beak</td>
      <td>Teeth</td>
      <td>Class</td>
   </tr>
   <tr>
      <td>3</td>
      <td>No</td>
      <td>Yes</td>
      <td>Many</td>
      <td>+</td>
   </tr>
   <tr>
      <td>4</td>
      <td>No</td>
      <td>Yes</td>
      <td>Many</td>
      <td>+</td>
   </tr>
   <tr>
      <td>3</td>
      <td>No</td>
      <td>Yes</td>
      <td>Few</td>
      <td>+</td>
   </tr>
   <tr>
      <td>5</td>
      <td>No</td>
      <td>Yes</td>
      <td>Many</td>
      <td>+</td>
   </tr>
   <tr>
      <td>5</td>
      <td>No</td>
      <td>Yes</td>
      <td>Few</td>
      <td>+</td>
   </tr>
   <tr>
      <td>5</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Many</td>
      <td>-</td>
   </tr>
   <tr>
      <td>4</td>
      <td>Yes</td>
      <td>Yes</td>
      <td>Many</td>
      <td>-</td>
   </tr>
   <tr>
      <td>5</td>
      <td>Yes</td>
      <td>No</td>
      <td>Many</td>
      <td>-</td>
   </tr>
   <tr>
      <td>4</td>
      <td>Yes</td>
      <td>No</td>
      <td>Many</td>
      <td>-</td>
   </tr>
   <tr>
      <td>4</td>
      <td>No</td>
      <td>Yes</td>
      <td>Few</td>
      <td>-</td>
   </tr>
</table>

<p>
Construct a decision tree using entropy as the impurity measure with
the <code>GrowTree(D, F)</code> algorithm, where $D$ is the set of
examples and $F$ is set of features. The algorithm determines which
feature to split on by choosing the one resulting in the highest
impurity reduction.  Impurity is a measure of homogeneity of a set of
instances; the more homogenous the instances, the lower the impurity.
When impurity the impurity of a set of instances $D$, denoted
$Imp(D)$, is 0, then the set is completely homogenous, i.e. the
instances are all of the same class.
</p>

<p>
To increase the generalisation of our decision tree, next apply the
PruneTree(T,D) algorithm using the following pruning set of labelled
data not seen during training:
</p>

<table>
<thead>
  <tr>
    <th>Instance</th>
    <th>Class</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td>p3 :  Length = 4 & Gills = no & Beak = no & Teeth = few</td>
    <td>+</td>
  </tr>
  <tr>
    <td>p4 :  Length = 4 & Gills = no & Beak = no & Teeth = many</td>
    <td>+</td>
  </tr>
  <tr>
    <td>n1 : Length = 5 & Gills = no & Beak = no & Teeth = few</td>
    <td>-</td>
  </tr>
  <tr>
    <td>n4 : Length = 4 & Gills = yes & Beak = yes & Teeth = few</td>
    <td>-</td>
  </tr>
</tbody>
</table>

<p>
Following the reduced-error pruning, complete the next contingency matrix
according to your results to observe the impact on the first given data.
</p><div style="border-style:solid; border-width:5px; border-color:#ff0000 #0000ff;">explanation:<br>When prunning the accuracy of the decision tree on the first data given will decrease</div><hr></p></div>

    <!-- ANSWER -->
    <ul class="wpProQuiz_questionList" data-question_id="12" data-type="cloze_answer">
<li class="wpProQuiz_questionListItem" data-pos="0">
<table>
  <tbody>
  <tr><td></td><td>Predicted+</td><td>Predicted-</td><td></td></tr>
  <tr>
    <td>Actual+</td>
    <td><span class="wpProQuiz_cloze"><input data-wordlen="2" type="text" value=""><span class="wpProQuiz_clozeCorrect" style="display: none;">(5)</span></span></td>
    <td><span class="wpProQuiz_cloze"><input data-wordlen="2" type="text" value=""><span class="wpProQuiz_clozeCorrect" style="display: none;">(0)</span></span></td>
    <td><span class="wpProQuiz_cloze"><input data-wordlen="2" type="text" value=""><span class="wpProQuiz_clozeCorrect" style="display: none;">(5)</span></span></td>
  </tr>
  <tr>
    <td>Actual-</td>
    <td><span class="wpProQuiz_cloze"><input data-wordlen="2" type="text" value=""><span class="wpProQuiz_clozeCorrect" style="display: none;">(1)</span></span></td>
    <td><span class="wpProQuiz_cloze"><input data-wordlen="2" type="text" value=""><span class="wpProQuiz_clozeCorrect" style="display: none;">(4)</span></span></td>
    <td><span class="wpProQuiz_cloze"><input data-wordlen="2" type="text" value=""><span class="wpProQuiz_clozeCorrect" style="display: none;">(5)</span></span></td>
  </tr>
  <tr>
    <td></td>
    <td><span class="wpProQuiz_cloze"><input data-wordlen="2" type="text" value=""><span class="wpProQuiz_clozeCorrect" style="display: none;">(6)</span></span></td>
    <td><span class="wpProQuiz_cloze"><input data-wordlen="2" type="text" value=""><span class="wpProQuiz_clozeCorrect" style="display: none;">(4)</span></span></td>
    <td><span class="wpProQuiz_cloze"><input data-wordlen="3" type="text" value=""><span class="wpProQuiz_clozeCorrect" style="display: none;">(10)</span></span></td>
  </tr>
  </tbody>
</table>
</li>
</ul>
  </div>


  <input type="button" name="back" value="Back" class="wpProQuiz_button wpProQuiz_QuestionButton" style="float: left !important; margin-right: 10px !important; display: none;">
  <input type="button" name="next" value="Next" class="wpProQuiz_button wpProQuiz_QuestionButton" style="float: right; display: none;">
  <div style="clear: both;"></div>
</li>

<!-- END OF QUESTION 12 -->

</ol>
          </div>
        </div>
        <script type="text/javascript">
          jQuery(document).ready(function($) {
      $('#wpProQuiz_2').wpProQuizFront({
              quizId: 2,
              mode: 1,
              globalPoints: 8,
              timelimit: 0,
              resultsGrade: [0],
              bo: 1031,
              qpp: 0,
              catPoints: {"4":1,"0":7}
              ,
              formPos: 0,
              lbn: "Finish quiz",
              json: {'12': {'points': 1, 'type': 'cloze_answer', 'id': 12, 'catId': 0, 'correct': ['5', '0', '5', '1', '4', '5', '6', '4', '10']}}      }
                                            );
          }
                                );
    </script>
      </div>
      <!-- end of .post-entry -->
      <!-- end of .post-data -->
      <div class="post-edit">
      </div>
    </div>
    <!-- end of #post-10 -->
  </div>
  <!-- end of #content-blog -->
  </div>
  <!-- end of #wrapper -->
  </div>
  <!-- end of #container -->

  <script type='text/javascript' src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'>
  </script>
  <!-- MathJax Latex Plugin installed -->
  <script type='text/javascript' src='resources/js/responsive_core_responsive-scripts.js?ver=1.2.4'>
  </script>
  <script type='text/javascript' src='resources/js/wp_ui_jquery_core.min.js?ver=1.10.3'>
  </script>
  <script type='text/javascript' src='resources/js/wp_ui_jquery_widget.min.js?ver=1.10.3'>
  </script>
  <script type='text/javascript' src='resources/js/wp_ui_jquery_mouse.min.js?ver=1.10.3'>
  </script>
  <script type='text/javascript' src='resources/js/wp_ui_jquery_sortable.min.js?ver=1.10.3'>
  </script>
  <script type='text/javascript'>
    /* <![CDATA[ */
    var WpProQuizGlobal = {"ajaxurl":"resources\/php\/wp_admin-ajax.php","loadData":"Loading","questionNotSolved":"You must answer this question.","questionsNotSolved":"You must answer all questions before you can completed the quiz.","fieldsNotFilled":"All fields have to be filled."}
        ;
    /* ]]> */
  </script>
  <script type='text/javascript' src='resources/js/wpProQuiz_front.min.js?ver=0.28'>
  </script>
  <script type='text/javascript' src='resources/js/wpProQuiz_jquery.ui.touch-punch.min.js?ver=0.2.2'>
  </script>
</body>
</html>