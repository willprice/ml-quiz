<!doctype html>
<!--[if !IE]>
<html class="no-js non-ie" lang="en-US">

<![endif]-->
<!--[if IE 7 ]>
<html class="no-js ie7" lang="en-US">

<![endif]-->
<!--[if IE 8 ]>
<html class="no-js ie8" lang="en-US">

<![endif]-->
<!--[if IE 9 ]>
<html class="no-js ie9" lang="en-US">

<![endif]-->
<!--[if gt IE 9]>
<!-->
<html class="no-js" lang="en-US">
  <!--
<![endif]-->
  <head>
    <meta charset="UTF-8"/>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>
      Machine Learning &#124; Quizzes
    </title>
    <link rel="profile" href="http://gmpg.org/xfn/11"/>
    <link rel="pingback" href="http://python.otuama.net/xmlrpc.php"/>
    <link rel="alternate" type="application/rss+xml" title="Machine Learning &raquo; Feed" href="http://python.otuama.net/feed/"/>
    <link rel="alternate" type="application/rss+xml" title="Machine Learning &raquo; Comments Feed" href="http://python.otuama.net/comments/feed/"/>
    <link rel='stylesheet' id='wpProQuiz_front_style-css' href='resources/css/wpProQuiz_front.min.css?ver=0.28' type='text/css' media='all'/>
    <link rel='stylesheet' id='responsive-style-css' href='resources/css/responsive_style.css?ver=1.9.3.4' type='text/css' media='all'/>
    <link rel='stylesheet' id='responsive-media-queries-css' href='resources/css/responsive_core_style.css?ver=1.9.3.4' type='text/css' media='all'/>
    <script type='text/javascript' src='resources/js/wp_jquery.js?ver=1.10.2'>
    </script>
    <script type='text/javascript' src='resources/js/wp_jquery-migrate.min.js?ver=1.2.1'>
    </script>
    <script type='text/javascript' src='resources/js/responsive_core_responsive-modernizr.js?ver=2.6.1'>
    </script>
    <link rel="EditURI" type="application/rsd+xml" title="RSD" href="http://python.otuama.net/xmlrpc.php?rsd"/>
    <link rel="wlwmanifest" type="application/wlwmanifest+xml" href="http://python.otuama.net/wp-includes/wlwmanifest.xml"/>
    <meta name="generator" content="WordPress 3.6.1"/>
    <!-- We need this for debugging -->
    <!-- Responsive 1.9.3.8 -->
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [['$','$']],
          displayMath: [ ['$$','$$'] ]
        }
      });
    </script>
    <script
    type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">

    </script>
  </head>
  <body class="home blog">
    <div id="container" class="hfeed">
      <div id="header">
    <div id="logo">

      </div>
      <!-- end of #logo -->
  </div>
  <!-- end of #header -->
  <div id="wrapper" class="clearfix">
    <div id="content-blog" class="grid col-940">
      <!-- Blog page title -->
      <h1>
      </h1>
      <div id="post-10" class="post-10 post type-post status-publish format-standard hentry category-distance-models category-quiz">
        <h2 class="entry-title post-title">
          <a href="#" rel="bookmark">
            Machine Learning Quiz
          </a>
        </h2>


        <!-- end of .post-meta -->
        <div class="post-entry">
          <div class="wpProQuiz_content" id="wpProQuiz_2">
            <h2>Machine Learning Quiz</h2>




          <div style="display: none;" class="wpProQuiz_results">
            <h4 class="wpProQuiz_header">
              Results
            </h4>
            <p class="wpProQuiz_time_limit_expired" style="display: none;">
              Time has elapsed
            </p>
            <div class="wpProQuiz_catOverview" style="display:none;">
              <h4>
                Categories
              </h4>
              <div style="margin-top: 10px;">
                <ol>
                  <li data-category_id="0">
                    <span class="wpProQuiz_catName">
                      Not categorized
                    </span>
                    <span class="wpProQuiz_catPercent">
                    </span>
                  </li>
                  <li data-category_id="4">
                    <span class="wpProQuiz_catName">
                      Distance
                    </span>
                    <span class="wpProQuiz_catPercent">
                    </span>
                  </li>
                </ol>
              </div>
            </div>
            <div>
              <ul class="wpProQuiz_resultsList">
                <li style="display: none;">
                  <div>
                  </div>
                </li>
              </ul>
            </div>
            <div style="margin: 10px 0px;">
              <input class="wpProQuiz_button" type="button" name="restartQuiz" value="Restart quiz">
              <input class="wpProQuiz_button" type="button" name="reShowQuestion" value="View questions">
            </div>
          </div>

          <div class="wpProQuiz_quizAnker" style="display: none;">
          </div>
          <div style="display: none;" class="wpProQuiz_quiz">
            <ol class="wpProQuiz_list">

<!-- START OF QUESTION 4 -->

<li class="wpProQuiz_listItem" style="display: none;">
  <h5 style="display: inline-block;" class="wpProQuiz_header"><span>4</span>. Question</h5>
  <div style="font-weight: bold; padding-top: 5px;">Category: 0.1: Beyond the scope of the book : See category for details</div>
  <div style="font-weight: bold; padding-top: 5px;">Category: hard (5)</div>

  <!-- QUESTION (SINGLE ANSWER) -->
  <div class="wpProQuiz_question" style="margin: 10px 0px 0px 0px;">

    <!-- QUESTION -->
    <div class="wpProQuiz_question_text"><p>Fill in the blanks</p></div>

    <!-- ANSWER -->
    <ul class="wpProQuiz_questionList" data-question_id="4" data-type="blank_answer">
<li class="wpProQuiz_questionListItem" data-pos="0"><div style="border-style:solid; border-width:5px; border-color:#ff0000 #0000ff;">comment:<br><p>
Tests the student's knowledge of neural networks and the circuit
interpretation of backpropagation.
</p>

<p>
Complexity 5 because it is out the scope of the book and requires
both calculation, and insight to derive the derivative of
$\sigma(x)$ in terms of itself
</p>

<p>
Author: Will Price
</p></div><hr><div style="border-style:solid; border-width:5px; border-color:#ff0000 #0000ff;">hint:<br>Blah</div><hr><div style="border-style:solid; border-width:5px; border-color:#ff0000 #0000ff;">workings:<br><p>
Solving for the intermediate values in forward propagation:
$$\begin{align}
  a_1 &= x_1w_1 = -1 \cdot 2 = -2 \\
  a_2 &= x_2w_2 = -2 \cdot -3 = 6 \\
  b &= x_1w_1 + x_2w_2 = -2 + 6 = 4 \\
  c &= (x_1w_1 + x_2w_2) + w_3= 4 + (-3) = 1 \\
  \hat{y} &= \sigma(x_1w_1 + x_2w_2 + w_3)= \sigma(1) = \frac{1}{1 + e^{-1}} = 0.731
\end{align}$$
</p>

<p>
Derivation of $\frac{d}{dx}(x + y)$
$$\begin{align}
    \frac{d}{dx}(x + y) &= 1
  \end{align}$$
</p>

<p>
Derivation of $\frac{d}{dx}(xy)$
$$\begin{align}
    \frac{d}{dx}(xy) &= y
  \end{align}$$
</p>

<p>
Derivation of $\frac{d}{dx}\sigma(x)$
$$\begin{align}
    \frac{d}{dx}\sigma(x) &= \frac{d}{dx}\left(\frac{1}{1 + e^{-x}}\right) \\
                          &= \frac{d}{du}\left(\frac{1}{u}\right) \cdot \frac{d}{dx}\left(1 + e^{-x}\right) \\
                          &= -(1 + e^{-x})^{-2} \cdot e^{-x} \cdot (-1) \\
                          &= (1 + e^{-x})^{-2} \cdot e^{-x} \\
                          &= \frac{1}{(1 + e^{-x})^2} \cdot e^{-x} \\
                          &= \frac{e^{-x}}{1 + e^{-x}} \cdot \frac{1}{1 + e^{-x}} \\
                          &= \frac{1 + e^{-x} - 1}{1 + e^{-x}} \cdot \frac{1}{1 + e^{-x}} \\
                          &= \left(\frac{1 + e^{-x}}{1 + e^{-x}} - \frac{1}{1 + e^{-x}}\right) \cdot \frac{1}{1 + e^{-x}} \\
                          &= \left(1 - \frac{1}{1 + e^{-x}}\right) \cdot \frac{1}{1 + e^{-x}} \\
                          &= \sigma(x) \left( 1 - \sigma(x) \right)
  \end{align}$$
  </p>

  <p>
  Solving for intermediate values in back propagation using the
  results above yields the following analytical solutions

  $$\frac{\partial L}{\partial c} = \frac{\partial \sigma(c)}{\partial c} = \sigma(c)(1 - \sigma(c))$$
  $$\frac{\partial c}{\partial b} = \frac{\partial (b + w_3)}{\partial b} = 1$$
  $$\frac{\partial c}{\partial w_3} = \frac{\partial (b + w_3)}{\partial w_3} = 1$$
  $$\frac{\partial b}{\partial a_1} = \frac{\partial (a_1 + a_2)}{\partial a_1} = 1$$
  $$\frac{\partial b}{\partial a_2} = \frac{\partial (a_1 + a_2)}{\partial a_2} = 1$$
  $$\frac{\partial a_1}{\partial w_1} = \frac{\partial (x_1w_1)}{\partial w_1} = x_1$$
  $$\frac{\partial a_2}{\partial w_2} = \frac{\partial (x_2w_2)}{\partial w_2} = x_2$$

  Substituting the values calculated in forward propagation:

  $$\frac{\partial L}{\partial c} = 0.731(1 - 0.731) = 0.200$$
  $$\frac{\partial c}{\partial b} = 1$$
  $$\frac{\partial c}{\partial w_3} = 1$$
  $$\frac{\partial b}{\partial a_1} = 1$$
  $$\frac{\partial b}{\partial a_2} = 1$$
  $$\frac{\partial a_1}{\partial w_1} = x_1 = -1$$
  $$\frac{\partial a_2}{\partial w_2} = x_2 = -2$$

  Using the chain rule we can find the desired partial derivatives:
  $$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial c}
                                      \frac{\partial c}{\partial b}
                                      \frac{\partial b}{\partial a_1}
                                      \frac{\partial a_1}{\partial w_1}
                                    = 0.200 \cdot 1 \cdot 1 \cdot -1 = -0.20
  $$
  $$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial c}
                                      \frac{\partial c}{\partial b}
                                      \frac{\partial b}{\partial a_2}
                                      \frac{\partial a_2}{\partial w_2}
                                    = 0.200 \cdot 1 \cdot 1 \cdot -2 = -0.40
  $$
  $$\frac{\partial L}{\partial w_3} = \frac{\partial L}{\partial c}
                                      \frac{\partial c}{\partial w_3}
                                    = 0.200 \cdot 1 = 0.20
  $$
  </p></div><hr><p>
Neural networks (NNs) are a class versatile models used to great
success in many different domains. A neural network is a directed
graph of nodes and edges. Nodes (neurons), either represent inputs
to the model or compute some function of their inputs. Edges
represent connections between adjacent neurons, they have a weight
associated with them. Most neurons compute the weighted sum of their
inputs followed by a non-linear function application (such as the
$\sigma$, or $\tanh$ function).
</p>

<p>
NNs can be trained using a technique called
<emph>backpropagation</emph>. The main idea behind backpropagation
is the rate of change of the loss of an output neuron can be
calculated with respect to the weights of the network through
repeated application of the chain rule. The weights of the network
can then be updated using gradient descent to reduce the loss.
</p>


<p>
One might expect backpropagation to be very complicated as the
number of neurons between an input and output neuron can be large,
leading to a very complex gradient function, however the chain rule
means we can decompose this into a series of simple calculations
evaluating the derivative of the output of each neuron with respect
to its input, these can then be combined through successive
multiplications to calculate the change in loss with respect to a
specific weight (see <a
href="https://en.wikipedia.org/wiki/Automatic_differentiation">automatic
differentiation</a>).
</p>

<p>
A simple network is presented in Fig 1 composed of two
input neurons and one output neuron. The output neuron computes the
weighted sum of its inputs and applies the sigmoid function to
produce its output.
</p>

<p>
We will investigate how neural networks function by forward
propagating (computing the output of the last layer of the network
from a set of inputs), then backpropagating through the network to
perform weight updates.
</p>

<p>
Mathematical functions can be thought of as gates in a circuit that
take inputs and produce an ouput, we will redraw the network in a
circuit style breaking down the network into gates that we use to
help perform back propagation.
</p>

<p>
The network in Fig 1 has been redrawn as a circuit in Fig 2. Forward
propagation is accomplished by applying the function specified in
the node to its inputs, the calculations necessary to compute the
output of each node are specified on the arrow between adjacent
neurons. For the initial weights values $(w_1, w_2, w_3) = (2, -3,
-3)$, and training example $(x_1, x_2) = (-1, -2)$, compute the
top labelled values (those computed during forward propagation) in
Fig 3.
</p>

<ul>
<li>$a_1$

<span class="wpProQuiz_cloze">
  <input data-wordlen="2" type="text" value="">
  <span class="wpProQuiz_clozeCorrect" style="display: none;">(-2)</span>
</span>
</li>
<li>$a_2$

<span class="wpProQuiz_cloze">
  <input data-wordlen="1" type="text" value="">
  <span class="wpProQuiz_clozeCorrect" style="display: none;">(6)</span>
</span>
</li>
<li>$b$

<span class="wpProQuiz_cloze">
  <input data-wordlen="1" type="text" value="">
  <span class="wpProQuiz_clozeCorrect" style="display: none;">(4)</span>
</span>
</li>
<li>$c$

<span class="wpProQuiz_cloze">
  <input data-wordlen="1" type="text" value="">
  <span class="wpProQuiz_clozeCorrect" style="display: none;">(1)</span>
</span>
</li>
<li>$\hat{y}$

<span class="wpProQuiz_cloze">
  <input data-wordlen="4" type="text" value="">
  <span class="wpProQuiz_clozeCorrect" style="display: none;">(0.73)</span>
</span>
</li>
</ul>
<p>
Having got a feel for how forward propagation works, we'll now
show you how to backpropagate. When training the network we need
some way of quantifying the difference in predicted output to the
expected output (ground truth). Loss functions are used to compute
this difference, some common examples of loss functions include
cross-entropy loss; and euclidean loss, the scaled sum of the
squared error of each output neuron. Our example only has a single
output neuron, so for the sake of simplicity we'll define loss as
the difference between the output and the expected output.

Our goal is to find
$$\nabla_{w} L = \left[
  \frac{\partial L}{\partial w_1},
  \frac{\partial L}{\partial w_2},
  \ldots,
  \frac{\partial L}{\partial w_k}
\right]$$

Each term in the vector defines how the loss changes as you
change a specific weight. $L$ represents a surface in a $k$
dimensional space (if we hold $x$ constant), $\nabla_{w} L$ is the
gradient at a specific point in that space defined by the current
weights of the network. Since we want to reduce the loss and make
our network more accurate, we can use gradient descent to traverse
the surface to a hollow, this is not necessarily a global minima,
but in practice gradient descent works well and produces networks
that successfully function in their task.
</p>

<p>
Let's look at a component of $\nabla_w L$, and figure out how to
derive it.
$$\frac{\partial L}{\partial w_1}$$
Recall this represents how the loss of the network changes as we
change weight $w_1$. We can decompose this into the product of
multiple partial derivatives, each partial derivative representing
how the output of a neuron changes with respect to its input, until
we reach the beginning of the network.
</p>

<p>
The input to each neuron has been labelled with a variable name on
Fig 2, the partial derivatives of the loss with respect to the weights are:
$$\frac{\partial L}{\partial w_1} = \frac{\partial L}{\partial b}
                                    \frac{\partial b}{\partial a_1}
                                    \frac{\partial a_1}{\partial w_1}$$
$$\frac{\partial L}{\partial w_2} = \frac{\partial L}{\partial b}
                                    \frac{\partial b}{\partial a_2}
                                    \frac{\partial a_2}{\partial w_2}$$
If we compute the analytical derivative of the output of each
neuron with respect to its input, we can construct an algorithm to
work backwards through the network to compute the gradient of the
loss with respect to the weights.
</p>

<p>
Compute the following derivatives:
  $$\frac{d}{dx}(x + y)$$
  $$\frac{d}{dx}(x \cdot y)$$
  $$\frac{d}{dx}(\sigma(y))$$
Where
  $$\sigma(x) = \frac{1}{1 + e^x}$$
</p>

<p>
Finally we can backpropagate and find out how changing the weights
affects the loss, starting from the output gate we can trace our
way back to an input weight by accumulating the derivative product
(i.e. incrementally build up the full partial derivative via the
chain rule). As we traverse a gate, we compute the partial
derivative and multiply the accumulator by the derivative until
reaching the input weight.
</p>

<p>
Following the above process, backpropagate the circuit in Fig 3.
</p>

What are the following partial derivatives?

<ul>
<li>$\frac{\partial L}{\partial w_1}$ =

<span class="wpProQuiz_cloze">
  <input data-wordlen="5" type="text" value="">
  <span class="wpProQuiz_clozeCorrect" style="display: none;">(-0.20)</span>
</span>
</li>
<li>$\frac{\partial L}{\partial w_2}$ =

<span class="wpProQuiz_cloze">
  <input data-wordlen="5" type="text" value="">
  <span class="wpProQuiz_clozeCorrect" style="display: none;">(-0.40)</span>
</span>
</li>
<li>$\frac{\partial L}{\partial w_3}$ =

<span class="wpProQuiz_cloze">
  <input data-wordlen="4" type="text" value="">
  <span class="wpProQuiz_clozeCorrect" style="display: none;">(0.20)</span>
</span>
</li>
</ul>
<div style="border-style:solid; border-width:5px; border-color:#ff0000 #0000ff;">explanation:<br>1 blank: See the workings for derivation<br>2 blank: See the workings for derivation<br>3 blank: See the workings for derivation<br>4 blank: See the workings for derivation<br>5 blank: See the workings for derivation<br>6 blank: See the workings for derivation<br>7 blank: See the workings for derivation<br>8 blank: See the workings for derivation</div><hr><br><figure><img src="img/nn-backprop-question-network.png" /><figcaption>Fig 1: Example neural network</figcaption></figure><br><figure><img src="img/nn-backprop-question-circuit.png" /><figcaption>Fig 2: Neural network from Fig 1 drawn as a circuit diagram,
showing how to calculate intermediate values in forward propagation</figcaption></figure><br><figure><img src="img/nn-backprop-question-circuit-answer-abstract.png" /><figcaption>Fig 3: Neural network from Fig 1 drawn as a circuit diagram,
showing how to calculate intermediate values in back propagation</figcaption></figure></li>
</ul>
  </div>


  <input type="button" name="back" value="Back" class="wpProQuiz_button wpProQuiz_QuestionButton" style="float: left !important; margin-right: 10px !important; display: none;">
  <input type="button" name="next" value="Next" class="wpProQuiz_button wpProQuiz_QuestionButton" style="float: right; display: none;">
  <div style="clear: both;"></div>
</li>

<!-- END OF QUESTION 4 -->

</ol>
          </div>
        </div>
        <script type="text/javascript">
          jQuery(document).ready(function($) {
      $('#wpProQuiz_2').wpProQuizFront({
              quizId: 2,
              mode: 1,
              globalPoints: 8,
              timelimit: 0,
              resultsGrade: [0],
              bo: 1031,
              qpp: 0,
              catPoints: {"4":1,"0":7}
              ,
              formPos: 0,
              lbn: "Finish quiz",
              json: {'4': {'points': 1, 'type': 'blank_answer', 'id': 4, 'catId': 0, 'correct': [['-2'], ['6'], ['4'], ['1'], ['0.73'], ['-0.20'], ['-0.40'], ['0.20']]}}      }
                                            );
          }
                                );
    </script>
      </div>
      <!-- end of .post-entry -->
      <!-- end of .post-data -->
      <div class="post-edit">
      </div>
    </div>
    <!-- end of #post-10 -->
  </div>
  <!-- end of #content-blog -->
  </div>
  <!-- end of #wrapper -->
  </div>
  <!-- end of #container -->

  <script type='text/javascript' src='http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML'>
  </script>
  <!-- MathJax Latex Plugin installed -->
  <script type='text/javascript' src='resources/js/responsive_core_responsive-scripts.js?ver=1.2.4'>
  </script>
  <script type='text/javascript' src='resources/js/wp_ui_jquery_core.min.js?ver=1.10.3'>
  </script>
  <script type='text/javascript' src='resources/js/wp_ui_jquery_widget.min.js?ver=1.10.3'>
  </script>
  <script type='text/javascript' src='resources/js/wp_ui_jquery_mouse.min.js?ver=1.10.3'>
  </script>
  <script type='text/javascript' src='resources/js/wp_ui_jquery_sortable.min.js?ver=1.10.3'>
  </script>
  <script type='text/javascript'>
    /* <![CDATA[ */
    var WpProQuizGlobal = {"ajaxurl":"resources\/php\/wp_admin-ajax.php","loadData":"Loading","questionNotSolved":"You must answer this question.","questionsNotSolved":"You must answer all questions before you can completed the quiz.","fieldsNotFilled":"All fields have to be filled."}
        ;
    /* ]]> */
  </script>
  <script type='text/javascript' src='resources/js/wpProQuiz_front.min.js?ver=0.28'>
  </script>
  <script type='text/javascript' src='resources/js/wpProQuiz_jquery.ui.touch-punch.min.js?ver=0.2.2'>
  </script>
</body>
</html>