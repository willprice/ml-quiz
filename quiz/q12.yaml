images: []
difficulty: "3"
reference: "5.1"
problem_type: training
source: |
  Machine Learning: Tree Models - Decision Trees
question: |
  <p>
  Defining the following features:
  $$
  \begin{align}
  Length &= [3,4,5]\\
  Gills  &= [\text{yes}, \text{no}]\\
  Beak   &= [\text{yes}, \text{no}]\\
  Teeth  &= [\text{many}, \text{few}]\\
  \end{align}
  $$
  and the following table of training samples:
  </p>

  <table>
     <tr>
        <td>Length</td>
        <td>Gills</td>
        <td>Beak</td>
        <td>Teeth</td>
        <td>Class</td>
     </tr>
     <tr>
        <td>3</td>
        <td>No</td>
        <td>Yes</td>
        <td>Many</td>
        <td>+</td>
     </tr>
     <tr>
        <td>4</td>
        <td>No</td>
        <td>Yes</td>
        <td>Many</td>
        <td>+</td>
     </tr>
     <tr>
        <td>3</td>
        <td>No</td>
        <td>Yes</td>
        <td>Few</td>
        <td>+</td>
     </tr>
     <tr>
        <td>5</td>
        <td>No</td>
        <td>Yes</td>
        <td>Many</td>
        <td>+</td>
     </tr>
     <tr>
        <td>5</td>
        <td>No</td>
        <td>Yes</td>
        <td>Few</td>
        <td>+</td>
     </tr>
     <tr>
        <td>5</td>
        <td>Yes</td>
        <td>Yes</td>
        <td>Many</td>
        <td>-</td>
     </tr>
     <tr>
        <td>4</td>
        <td>Yes</td>
        <td>Yes</td>
        <td>Many</td>
        <td>-</td>
     </tr>
     <tr>
        <td>5</td>
        <td>Yes</td>
        <td>No</td>
        <td>Many</td>
        <td>-</td>
     </tr>
     <tr>
        <td>4</td>
        <td>Yes</td>
        <td>No</td>
        <td>Many</td>
        <td>-</td>
     </tr>
     <tr>
        <td>4</td>
        <td>No</td>
        <td>Yes</td>
        <td>Few</td>
        <td>-</td>
     </tr>
  </table>

  <p>
  Construct a decision tree using entropy as the impurity measure with
  the <code>GrowTree(D, F)</code> algorithm, where $D$ is the set of
  examples and $F$ is set of features. The algorithm determines which
  feature to split on by choosing the one resulting in the highest
  impurity reduction.  Impurity is a measure of homogeneity of a set of
  instances; the more homogenous the instances, the lower the impurity.
  When impurity the impurity of a set of instances $D$, denoted
  $Imp(D)$, is 0, then the set is completely homogenous, i.e. the
  instances are all of the same class.
  </p>

  <p>
  To increase the generalisation of our decision tree, next apply the
  PruneTree(T,D) algorithm using the following pruning set of labelled
  data not seen during training:
  </p>

  <table>
  <thead>
    <tr>
      <th>Instance</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>p3 :  Length = 4 & Gills = no & Beak = no & Teeth = few</td>
      <td>+</td>
    </tr>
    <tr>
      <td>p4 :  Length = 4 & Gills = no & Beak = no & Teeth = many</td>
      <td>+</td>
    </tr>
    <tr>
      <td>n1 : Length = 5 & Gills = no & Beak = no & Teeth = few</td>
      <td>-</td>
    </tr>
    <tr>
      <td>n4 : Length = 4 & Gills = yes & Beak = yes & Teeth = few</td>
      <td>-</td>
    </tr>
  </tbody>
  </table>

  <p>
  Following the reduced-error pruning, complete the next contingency matrix
  according to your results to observe the impact on the first given data.
  </p>

answer_type: cloze_answer
answers:
  answer:
    - "5 | 0  | 5"
    - "------------"
    - "1 | 4 | 5"
    - "------------"
    - "6 | 4 | 10"
  explanation: |
    When prunning the accuracy of the decision tree on the first data given will decrease

workings: |
  <div style="display: none">
  $$\(
  \DeclareMathOperator{\imp}{Imp}
  \)$$
  </div>
  <p>
  GrowTree(D,F) is a divide-and-conquer algorithm that in our case given the
  D instance space containing 10 samples and the F set of features will return
  a feature tree T with labbelled leaves, assuring that each labeled leaf is
  is homogeneous and each split is given by BestSplit-Class(D,F) algorithm.
  </p>

  <pre>
  <code>
  The GrowTree(D,F) is as follows:

  Input : data D; features D.
  Output: feature tree T with labbelled leaves.
  if Homogeneous(D) then return Label(D);
  S<-BestSplit(D,F);
  split D into subsets D(i) according to the literals in S;
  for each i do
    if D(i) != {} then
      T(i) <- GrowTree(D(i),F)
      else
        T(i) is a leaf labelled with Label(D)
  end
  return a tree whose root is labelled with S and whose children are T(i)
  </code>
  </pre>


  <p>
  The BestSplit-class algorithm chooses as the best feature to spilt on, the one
  that achieves the maximum reduction in impurity and in the case of
  using entropy as the impurity metric, the maximum information gain.
  The gain in impurity from a parent node $D$ to a set of children
  $\{D_1, ..., D_l\}$ is defined as $\imp(D) - \imp(\{D_1,...,D_l\})$.
  </p>

  <p>
  When choosing the best feature to split, we can simply choose the
  feature that produces the minimum weighted impurity since, the
  impurity of the parent is constant when testing out all the different
  feature splits.
  </p>

  <p>
  Splitting the data set based on the feature on the left results in the
  child leaf counts on the right

  \begin{align}
  Length &= 3           \rightarrow [2+, 0-]\\
  Length &= 4           \rightarrow [1+, 3-]\\
  Length &= 5           \rightarrow [2+, 2-]\\
  Gills  &= \text{yes}  \rightarrow [0+, 4-]\\
  Gills  &= \text{no}   \rightarrow [5+, 1-]\\
  Beak   &= \text{yes}  \rightarrow [5+, 3-]\\
  Beak   &= \text{no}   \rightarrow [0+, 2-]\\
  Teeth  &= \text{many} \rightarrow [3+, 4-]\\
  Teeth  &= \text{few}  \rightarrow [2+, 1-]\\
  \end{align}
  </p>

  <p>
  Using these counts and entropy as our purity measure we can determine
  which feature produces the best split by finding the feature that
  produced the minimum weight sum of impurities of the child leaves.
  </p>

  <p>
  We will abbreviate features and values for brevity, length becomes
  $L$, gills becomes $G$, beak becomes $B$, and so on.

  <p>
  <b>Split on Length</b><br />
  For $L = 3$ we only have positive instances meaning the leaf will
  be pure so therefore we have 

  $$\imp(D_{L= 3}) = 0$$

  $$\imp(D_{L = 4}) = -(1/4) \log(1/4) - (3/4) \log(3/4) = 0.5 + 0.31 = 0.81$$

  For $L = 5$, the leaf has the same number of each class so impurity as
  it a maximum: $\imp(D_{L=5}) = 1$

  Calculating the weighted sum of impurity:

  $$\imp(\{D_{L = 3}, D_{L = 4}, D_{L = 5}\}) = 2/10 \cdot 0 + 4/10 \cdot 0.81 + 4/10 \cdot 1 = 0.72 $$
  </p>


  <p>
  <b>Split on Gills</b><br />
  For feature $G = Y$ we only have negative instances so we will have again
  an entropy of 0.
  <br />

  $$\imp(D_{G=N}) = -(5/6) \log(5/6) - (1/6) \log(1/6)$$

  Calculating the weighted sum of impurity:

  $$\imp(\{D_{G=Y}, D_{G=N}\}) = 4/10 \cdot 0 + 6/10 \cdot(-(5/6)\log(5/6)-(1/6)\log(1/6)) = 0.39.  $$
  </p>

  <p>
  <b>Split on Beak</b><br />
  $$
  \imp(\{D_{B=Y}, D_{B=N}\}) = 8/10 \cdot(-(5/8) \log(5/8) - (3/8) \log(3/8)) + 2/10 \cdot 0 = 0.76
  $$
  </p>

  <p>
  <b>Split on Teeth</b><br />
  $$
  \begin{align}
  \imp(\{D_{T=F}, D_{T=M}\}) = & 7/10 \cdot(-(3/7) \log(3/7) - (4/7) \log(4/7)) + {} \\
                           & 3/10 \cdot(-(2/3) \log(2/3) - (1/3) \log(1/3)) \\
                           = &0.97
  \end{align}
  $$
  </p>

  <p>
  As we already established that the BestSplit-class will choose the feature that
  achieves the minimum impurity we can therefore say that 'Gills' will be the first
  best feature to split on. The rest of the tree can be derived in a similar fashion.
  </p>

  <p>
  Gills = no will be homogeneous over the negative class hence it will
  be labeled that negative. In a similar vein, we will repeat the
  labelling process on the Gills = yes branch until all the leafs
  obtained until the leaves are all labelled. From top to bottom, the
  decision tree will be in the end split in order on the following
  features: gills, length, teeth.
  </p>

  <p>
  However, that implies that we will in the end overfit our decision tree for the
  data provided.To avoid this we can either stop our feature tree construction earlier,
  process known as pre-prunning or we can otherwise reasses our splits in a bottom-up
  manner provided a new set of data that was not part of our training procedure.
  </p>

  <p>
  The latter will be realised using the PruneTree algorithm that takes as input
  a labelled decision tree and a new labelled set of instances and outputs a
  pruned tree T' as follows:
  </p>

  <pre>
  <code>
  PruneTree(T,D) is as follows:

  Input : decision tree D; labelled data D;
  Output: pruned tree T'.
  for every internal node N of T, starting form the bottom do:
    T{N} <- subtree of T rooted at N;
    D{N} <- {x from D|x is covered by N}
    if accuracy of T{N} over D{N} is worse than majority class in D{N} then
      replace T{N} in T be a leaf labelled with the majority class in D{N}
      end
  end
  return pruned version of T
  </code>
  </pre>

  <p>
  Using the new data given teeth feature will classify a positive instances
  as being negative in the case of teeth = few. However, if we were to take
  the majority class of its parent we would be able to classify it correctly
  having therefore a higher accuracy that will cause that subtree to be pruned.
  Is it also revelant to say that now that all conditional branches of length
  classify their instances as being positive we can prune that subtree as well,
  keeping in the end only the split on the gills feature.
  </p>

hint: |
  <p>
  Follow the given algorithms taking into consideration that finding the best
  split will require finding the feature that achieves the minimum entropy/impurity
  while at each added split the number of instances will reduce.
  </p>

comments: |
  <p>
  Problem requires the understanding of basic algorithms used in the construction of
  decision trees, while also introducing the problem of overfitting which can be solved
  through prunning.
  </p>

  <p>
  Complexity 3 because it requires extensive computations for determining the best
  feature to split on and also the decision tree accuracy on the new data presented.
  </p>
