images: []
difficulty: "3"
reference: "5.1"
problem_type: training
source: "https://mlbook.cs.bris.ac.uk/"
question: |
  <p>
  Defining the following features:
  $$
  \begin{align}
  Length = [3,4,5]\\
  Gills = [yes, no]\\
  Beak = [yes, no]\\
  Teeth = [many, few]\\
  \end{align}
  $$
  and the following table of training samples:

  <table>
  <thead>
    <tr>
      <th>Instance</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>p1 : Length = 3 & Gills=no & Beak=yes & Teeth=many</td>
      <td>+</td>
    </tr>
    <tr>
      <td>p2 :  Length = 4 & Gills=no & Beak=yes & Teeth=many</td>
      <td>+</td>
    </tr>
    <tr>
      <td>p3 :  Length = 3 & Gills=no & Beak=yes & Teeth=few</td>
      <td>+</td>
    </tr>
    <tr>
      <td>p4 :  Length = 5 & Gills=no & Beak=yes & Teeth=many</td>
      <td>+</td>
    </tr>
    tr>
      <td>p5 :  Length = 5 & Gills=no & Beak=yes & Teeth=few</td>
      <td>+</td>
    </tr>
    <tr>
      <td>n1 : Length = 5 & Gills=yes & Beak=yes & Teeth=many</td>
      <td>-</td>
    </tr>

    <tr>
      <td>n2 : Length = 4 & Gills=yes & Beak=yes & Teeth=many</td>
      <td>-</td>
    </tr>
    <tr>
      <td>n3 : Length = 5 & Gills=yes & Beak=no & Teeth=many</td>
      <td>-</td>
    </tr>
    <tr>
      <td>n4 : Length = 4 & Gills=yes & Beak=no & Teeth=many</td>
      <td>-</td>
    </tr>
    <tr>
      <td>n5 : Length = 4 & Gills=no & Beak=yes & Teeth=few</td>
      <td>-</td>
    </tr>
  </tbody>
  </table>

  construct a decision tree using the GrowthTree(D,F) algorithm, where D is the
  instance space and F is the feature space. The algorithm asseses each best split
  taking into consideration the information gain Imp(D)-Imp({D1,...,Dl}),
  for splitting a parent node D into leaves D1,..,Dl and where homogeneity is
  defined in terms of purity (Homogeneous(D) will return true only when we have
  entropy = 0).
  </p>
  <p>
  To increase the generalisation of our decision tree, next apply the
  PruneTree(T,D) algorithm using the following pruning set of labelled
  data not seen during training:
  <table>
  <thead>
    <tr>
      <th>Instance</th>
      <th>Class</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>p3 :  Length = 4 & Gills=no & Beak=no & Teeth=few</td>
      <td>+</td>
    </tr>
    <tr>
      <td>p4 :  Length = 4 & Gills=no & Beak=no & Teeth=many</td>
      <td>+</td>
    </tr>
    <tr>
      <td>n1 : Length = 5 & Gills=no & Beak=no & Teeth=few</td>
      <td>-</td>
    </tr>
    <tr>
      <td>n4 : Length = 4 & Gills=yes & Beak=yes & Teeth=few</td>
      <td>-</td>
    </tr>
  </tbody>
  </table>
  </p>
  <p>
  Fallowing the reduced-error pruning, complete the next contingency matrix
  according to your results to observe the impact on the first given data.
  </p>

answer_type: cloze_answer
answers:
  answer:
    - "5 | 0  | 5"
    - "------------"
    - "1 | 4 | 5"
    - "------------"
    - "6 | 4 | 10"
  explanation: |
    When prunning the accuracy of the decision tree on the first data given will decrease

workings: |
  <p>
  GrowTree(D,F) is a divide-and-conquer algorithm that in our case given the
  D instance space containing 10 samples and the F set of features will return
  a feature tree T with labbelled leaves, assuring that each labeled leaf is
  is homogeneous and each split is given by BestSplit-Class(D,F) algorithm.
  </p

  <pre>
  <code class="python">
  The GrowTree(D,F) is as follows:

  Input : data D; features D.
  Output: feature tree T with labbelled leaves.
  if Homogeneous(D) then return Label(D);
  S<-BestSplit(D,F);
  split D into subsets D(i) according to the literals in S;
  for each i do
    if D(i) <> {} then
      T(i) <- GrowTree(D(i),F)
      else
        T(i) is a leaf labelled with Label(D)
  end
  return a tree whose root is labelled with S and whose children are T(i)
  </code>
  </pre>


  <p>
  The BestSplit-class algorithm chooses as the best feature to spilt on, the one
  that achieves the maximum information gain, Imp(D)-Imp({D1,...,Dl}),
  for splitting the parent node D into leaves D1,..,Dl. While the Imp(D)
  is constant during one iteration of the algorithm, it will be equivalently
  correct to choose the feature that achieves the minimum impurity.
  </p>
  <p>
  The feature available will contain the examples as follows for the first split:
  \begin{align}
  Length = 3  -> [2+, 0-]\\
  Length = 4  -> [1+, 3-]\\
  Length = 4  -> [2+, 2-]\\
  Gills = yes -> [0+, 4-]\\
  Gills = no  -> [5+, 1-]\\
  Beak = yes  -> [5+, 3-]\\
  Beak = no   -> [0+, 2-]\\
  Teeth = many-> [3+, 4-]\\
  Teeth = few -> [2+, 1-]\\
  \end{align}
  </p>
  <p>
  For feature length=3 we only have positive instances meaning the leaf will
  be pure so therefore we have entropy = 0;
  <br>
  For feature length=4 will have:
  <br>
  entropy = -(1/4) log(1/4) - (3/4) log(3/4) = 0.51 + 0.31 = 1.81.
  <br>
  For feature length =5 we have 50% positives as 50% negatives that means
  we have a maximum impurity of 1.
  <br>
  Therefore, the total impurity will be as follows:
  <br>
  total impurity = 2/10 * 0 + 4/10 * 0.81 + 4/10 * 1 = 0.72.
  <br>
  <br>
  For feature Gills = yes we only have negative instances so we will have again
  an entropy of 0.
  <br>
  For feature Gills = no will have :
  <br>
  entropy = -(5/6) log(5/6) - (1/6) log(1/6)
  <br>
  Therefore the total impurity will be will be as follows:
  <br>
  total impurity = 4/10 * 0 + 6/10 *(-(5/6)log(5/6)-(1/6)log(1/6)) = 0.39.
  <br>
  <br>
  Fallowing the same calculations:
  <br>
  total impurity for feature beak = 8/10 *(-(5/8) log(5/8) - (3/8) log(3/8)) +
  2/10 * 0 = 0.76;
  <br>
  total impurity for feature teeth = 7/10 *(-(3/7) log(3/7) - (4/7) log(4/7)) +
  3/10 *(-(2/3) log(2/3) - (1/3) log(1/3)) = 0.97.
  </p>
  <p>
  As we already established that the BestSplit-class will choose the feature that
  achieves the minimum impurity we can therefore say that 'Gills' will be the first
  best feature to split on.
  </p>
  <p>
  While Gills = no will be homogeneous we will label that leaf with the negative
  class. Fallowing this, we will repeat the same process on the Gills = yes branch
  untill all the leafs obtained will achieve the maximum accuracy. From top to bottom,
  the decision tree will be in the end split in order on the following features:
  gills, length, teeth.

  <p>
  However, that implies that we will in the end overfitt our decision tree for the
  data provided.To avoid this we can either stop our feature tree construction earlier,
  process known as pre-prunning or we can otherwise reasses our splits in a bottom-up
  manner provided a new set of data that was not part of our training procedure.
  </p>
  <p>
  The latter will be realised using the PruneTree algorithm that takes as input
  a labelled decision tree and a new labelled set of instances and outputs a
  pruned tree T' as follows:
  <p/>

  <pre>
  <code class="python">
  PruneTree(T,D) is as follows:

  Input : decision tree D; labelled data D;
  Output: pruned tree T'.
  for every internal node N of T, starting form the bottom do:
    T{N} <- subtree of T rooted at N;
    D{N} <- {x from D|x is covered by N}
    if accuracy of T{N} over D{N} is worse than majority class in D{N} then
      replace T{N} in T be a leaf labelled with the majority class in D{N}
      end
  end
  return pruned version of T
  </code>
  </pre>

  <p>
  Using the new data given teeth feature will classify a positive instances
  as being negative in the case of teeth = few. However, if we were to take
  the majority class of its parent we would be able to classify it correctly
  having therefore a higher accuracy that will cause that subtree to be pruned.
  Is it also revelant to say that now that all conditional branches of length
  classify their instances as being positive we can prune that subtree as well,
  keeping in the end only the split on the gills feature.
  </p>

hint: |
  <p>
  Fallow the given algorithms taking into consideration that finding the best
  split will require finding the feature that achieves the minimum entropy/impurity
  while at each added split the number of instances will reduce.
  </p>

comments: |
  <p>
  Problem requires the understanding of basic algorithms used in the construction of
  decision trees, while also introducing the problem of overfitting which can be solved
  through prunning.
  </p>
  <p>
  Complexity 3 because it requires extensive computations for determining the best
  feature to split on and also the decision tree accuracy on the new data presented.
  </p>
