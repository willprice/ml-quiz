images: []
difficulty: "3"
reference: "7.1"
problem_type: training
source: |
  Machine Learning: Linear Models - The least-squares method
question: |
  Given the following training examples:

  <table>
  <thead>
    <tr>
      <th>x</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <td>6</td>
      <td>4</td>
    </tr>
    <tr>
      <td>7</td>
      <td>4</td>
    </tr>
    <tr>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <td>9</td>
      <td>4</td>
    </tr>
    <tr>
      <td>10</td>
      <td>5</td>
    </tr>
  </tbody>
  </table>

  <p>
  Determine the best fitting polynomial $\hat{y} = a_0 +a_1x + a_2x^{2}$, using the matrix form least squares method.
  </p>

  <table>
  <thead>
    <tr>
      <th>x</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2</td>
      <td>5</td>
    </tr>
    <tr>
      <td>4</td>
      <td>6</td>
    </tr>
  </tbody>
  </table>

  <p>
  The data points above are outliers, train a new regressor $\hat{y}'$
  by including the outliers in the training data and compare the
  coefficients before and after adding the outliers.
  </p>

  <p>
  Given the new data point $(8, 3)$, determine the prediction of
  $\hat{y}$ and $\hat{y}'$, then calculate the residual for each
  regressor, finally compute the difference between the residuals and
  select the correct difference of residuals below:
  </p>

answer_type: single
answers:
  - correctness: "-"
    answer: "0"
  - correctness: "-"
    answer: "3"
  - correctness: "+"
    answer: "2.7"
    explanation: |
      "the outliers were expected to increase the residual"
  - correctness: "-"
    answer: "1.6"

workings: |
  <p>
  Finding the model parameters amounts to finding the best fitting line given the
  data involves minimising the distance measure from the points to the line.
  </p>

  <p>
  The form of a polynomial of degree 2:

  $$y = a_0 +a_1x + a_2x{2}$$

  The sum squared of residuals of training data $X$ is

  $$R(a_0, a_1, a_2) = \sum_{i = 1}^N \left[ (y_i - (a_0 +a_1 x_i + a_2 x_i^{2}))^{2} \right] = ||\mathbf{y} - \mathbf{X} \cdot \mathbf{a}||^{2}$$
  where:

  $$\mathbf{y} =
  \begin{pmatrix}
    y_1\\
    .\\
    .\\
   y_N\\
  \end{pmatrix}$$

  $$\mathbf{X} =
  \begin{pmatrix}
    1 & x_1 & x_1{2}\\
    . & . & .\\
    . & . & .\\
    1 & x_N & x_N{2}\\
  \end{pmatrix}$$

  $$\mathbf{a} =
  \begin{pmatrix}
    a_0\\
    a_1\\
    a_2\\
  \end{pmatrix}$$.

  <p>
  $\mathbf{a_{LS}}$ is the vector $\mathbf{a}$ obtained by minimizing
  the sum squared of the residuals, the formula for obtaining
  $\mathbf{a_{LS}}$ is $\mathbf{a_{LS}} = (\mathbf{X}^T \mathbf{X})^{-1}
  \mathbf{X}^{T} \mathbf{y}$. This can be derived by considering partial
  derivatives of the sum of squared residuals with respect to each
  individual residual component and forming the gradient of
  $\mathbf{a_{LS}}$.
  </p>

  <p>
  Given the set of annotated training examples in the form $(x, y)$:

  $$\{(1,1), (3,2), (5,2), (6,4), (7,4), (8,3), (9,4), (10,5)\}$$

  We will derive $a_{LS}$
  </p>

  <p>
  $$\mathbf{y} =
  \begin{pmatrix}
    1\\
    2\\
    2\\
    4\\
    4\\
    3\\
    4\\
    5\\
  \end{pmatrix}$$.

  $$\mathbf{X} =
  \begin{pmatrix}
    1 & 1 & 1{2}\\
    1 & 3 & 3{2}\\
    1 & 5 & 5{2}\\
    1 & 6 & 6{2}\\
    1 & 7 & 7{2}\\
    1 & 8 & 8{2}\\
    1 & 9 & 9{2}\\
    1 & 10 & 10{2}\\
  \end{pmatrix}$$

  $$\mathbf{a} =
  \begin{pmatrix}
    a_0\\
    a_1\\
    a_2\\
  \end{pmatrix}$$.
  So,
  $$\mathbf{X{T}X} =
  \begin{pmatrix}
    1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
    1 & 3 & 5 & 6 & 7 & 8 & 9 & 10\\
    1{2} & 3{2} & 5{2} & 6{2} & 7{2} & 8{2} & 9{2} & 10{2}\\
  \end{pmatrix}
  \begin{pmatrix}
    1 & 1 & 1^{2}\\
    1 & 3 & 3^{2}\\
    1 & 5 & 5^{2}\\
    1 & 6 & 6^{2}\\
    1 & 7 & 7^{2}\\
    1 & 8 & 8^{2}\\
    1 & 9 & 9^{2}\\
    1 & 10 & 10^{2}\\
  \end{pmatrix}
  =
  \begin{pmatrix}
    8 & 49 & 365\\
    49 & 365 & 2953\\
    365 & 2953 & 25061\\
  \end{pmatrix}$$

  So after inverting $X^{T} X$ we obtained:

  $$
  \mathbf{a_{LS}} = (X^{T}X)^{-1}X^{T} =
  \begin{pmatrix}
    0.6009\\
    0.4395\\
    -0.0037\\
  \end{pmatrix}
  $$

  $$\hat{y} = 0.60 + 0.43x -0.0037x^{2}$$
  </p>

  <p>
  Introducing the outliers and retraining the classifier to produce a
  new regressor $\hat{y}'$ determined by $\mathbf{a}_{LS'}$


  $$
  \mathbf{a_{LS'}} = 
  \begin{pmatrix}
    1.3851 & -0.5259 & 0.0417\\
    -0.5259 & 0.2417 & -0.0209\\
    0.0417 & -0.0209 & 0.0019\\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
    1 & 3 & 5 & 6 & 7 & 8 & 9 & 10 & 2 & 4\\
    1^{2} & 3^{2} & 5^{2} & 6^{2} & 7^{2} & 8^{2} & 9^{2} & 10^{2} & 2^{2} & 4^{2}\\
  \end{pmatrix}
  \cdot
  \begin{pmatrix}
    1\\
    2\\
    2\\
    4\\
    4\\
    3\\
    4\\
    5\\
    5\\
    6\\
  \end{pmatrix}
  =
  \begin{pmatrix}
    2.1825\\
    0.3905\\
    -0.0190\\
  \end{pmatrix}
  $$

  $$\hat{y}' = 2.182 + 0.39x -0.019x^{2}$$
  </p>

  <p>
  So for (8,3) we will predict $\hat{y}$ = 3.7968 and $\hat{y}'$ = 6.518.

  $$|\hat{y} - y| = 0.7968$$
  $$|\hat{y}' - y| = 3.518$$
  $$\left| |\hat{y} - y| - |\hat{y}' - y| \right| = 2.7212$$ 

  where

  $$|\hat{y} - y| > |\hat{y}' - y|$$ 

  which we were
  expecting as the least-square method is sensible to outliers.
  </p>
hint: |
  <p>
  Generalise the definition of the linear function to be under the following form
  $y = a_0 +a_1x + a_2x^{2}$. Then compute the slope and the y-intercept trying to
  minimise the residual.
  </p>

comments: |
  The problem's purpose is to explose the least-square metod sensitivity to outliers,
  but also explore the least-square method using the matrix form with increased
  complexity given by the polynomial definition of the feature dependency.
