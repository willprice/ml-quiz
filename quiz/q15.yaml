images: []
difficulty: "3"
reference: "7.1"
problem_type: training
source: "https://mlbook.cs.bris.ac.uk/"
question: |
  Given the following instance table:
  <table>
  <thead>
    <tr>
      <th>x</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td>3</td>
      <td>2</td>
    </tr>
    <tr>
      <td>5</td>
      <td>2</td>
    </tr>
    <tr>
      <td>6</td>
      <td>4</td>
    </tr>
    tr>
      <td>7</td>
      <td>4</td>
    </tr>
    <tr>
      <td>8</td>
      <td>3</td>
    </tr>
    <tr>
      <td>9</td>
      <td>4</td>
    </tr>
    <tr>
      <td>10</td>
      <td>5</td>
    </tr>
  </tbody>
  </table>
  \begin{align}

  \end{align}
  determine the best fitting polynomial $y = a_0 +a_1x + a_2x{2}$, using the matrix form.
  <br>
  After introducing 2 more samples:
  <table>
  <thead>
    <tr>
      <th>x</th>
      <th>y</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>2</td>
      <td>5</td>
    </tr>
    <tr>
      <td>4</td>
      <td>6</td>
    </tr>
  </tbody>
  </table>
  compute the new best fitting polynomial and compare your results, taking into
  consideration that the 2 added instances are outliers.

  <br>
  Now compute the difference between the predicted y and given y in each case for
  the sample (8,3). Then compute their difference and choose the correct one from
  the list below:

answer_type: single
answers:
  - correctness: "-"
    answer: "0"
  - correctness: "-"
    answer: "3"
  - correctness: "+"
    answer: "2.7"
    explanation: |
      "the outliers were expected to increase the residual"
  - correctness: "-"
    answer: "1.6"

workings: |
  <p>
  Finding the model parameters amounts to finding the best fitting line given the
  data that implies minimising the distance measure from the points to the line,
  concept also used in the Least-Squares Method.
  </p>

  <p>
  Therefore having the following polynomial definition of the fitting line:
  \begin{align}
  y = a_0 +a_1x + a_2x{2}\\
  \end{align}
  we will try to minimise the residual described as below:
  </p>

  \begin{align}
  R(a_0, a_1, a_2) &= \sum_{i \in N}\left[ (y_i - (a_0 +a_1x + a_2x{2})){2} \right] &= ||y-Xa||{2}\\
  \end{align}
  where:

  $$\mathbf{y} =
  \begin{pmatrix}
    y_1\\
    .\\
    .\\
   y_N\\
  \end{pmatrix}$$

  $$\mathbf{X} =
  \begin{pmatrix}
    1 & x_1 & x_1{2}\\
    . & . & .\\
    . & . & .\\
    1 & x_N & x_N{2}\\
  \end{pmatrix}$$

  $$\mathbf{a} =
  \begin{pmatrix}
    a_0\\
    a_1\\
    a_2\\
  \end{pmatrix}$$.

  <p>
  To solve the least squares we will use the following deductions:

  \begin{align}
  \text{minimise } R(a) = ||y - Xa_L||{2} => ||y - Xa_L||{2} =0 => y - Xa_L = 0 =>
  Xa_L = y => X{T}Xa_L = X{T}y\\
  => a_L = (X{T}X){-1}X{T}y\\
  \end{align}

  </p>

  <p>
  Having the instance space:
  <br>
  $(1,1) (3,2) (5,2) (6,4) (7,4) (8,3) (9,4) (10,5)$
  <br>
  we will constuct the following matrices:
  </p>

  $$\mathbf{y} =
  \begin{pmatrix}
    1\\
    2\\
    2\\
    4\\
    4\\
    3\\
    4\\
    5\\
  \end{pmatrix}$$.

  $$\mathbf{X} =
  \begin{pmatrix}
    1 & 1 & 1{2}\\
    1 & 3 & 3{2}\\
    1 & 5 & 5{2}\\
    1 & 6 & 6{2}\\
    1 & 7 & 7{2}\\
    1 & 8 & 8{2}\\
    1 & 9 & 9{2}\\
    1 & 10 & 10{2}\\
  \end{pmatrix}$$

  $$\mathbf{a} =
  \begin{pmatrix}
    a_0\\
    a_1\\
    a_2\\
  \end{pmatrix}$$.
  So,
  $$\mathbf{X{T}X} =
  \begin{pmatrix}
    1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
    1 & 3 & 5 & 6 & 7 & 8 & 9 & 10\\
    1{2} & 3{2} & 5{2} & 6{2} & 7{2} & 8{2} & 9{2} & 10{2}\\
  \end{pmatrix}
  \begin{pmatrix}
    1 & 1 & 1{2}\\
    1 & 3 & 3{2}\\
    1 & 5 & 5{2}\\
    1 & 6 & 6{2}\\
    1 & 7 & 7{2}\\
    1 & 8 & 8{2}\\
    1 & 9 & 9{2}\\
    1 & 10 & 10{2}\\
  \end{pmatrix}
  =
  \begin{pmatrix}
    8 & 49 & 365\\
    49 & 365 & 2953\\
    365 & 2953 & 25061\\
  \end{pmatrix}$$
  <p>
  So after inverting the $X{T}X$ we obtained:
  </p>

  $$\mathbf{a_LS = (X{T}X){-1}X{T}} =
  \begin{pmatrix}
    0.6009\\
    0.4395\\
    -0.0037\\
  \end{pmatrix}$$.

  \begin{align}
  => y = 0.60 + 0.43*x -0.0037*x{2}\\
  \end{align}

  However, introducing the new 2 points will have:
  \begin{align}
  a_LS = (X{T}X){-1}X{T}y=\\
  \end{align}

  $$\mathbf{} =
  \begin{pmatrix}
    1.3851 & -0.5259 & 0.0417\\
    -0.5259 & 0.2417 & -0.0209\\
    0.0417 & -0.0209 & 0.0019\\
  \end{pmatrix}
  *
  \begin{pmatrix}
    1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1 & 1\\
    1 & 3 & 5 & 6 & 7 & 8 & 9 & 10 & 2 & 4\\
    1{2} & 3{2} & 5{2} & 6{2} & 7{2} & 8{2} & 9{2} & 10{2} & 2{2} 4{2}\\
  \end{pmatrix}
  *
  \begin{pmatrix}
    1\\
    2\\
    2\\
    4\\
    4\\
    3\\
    4\\
    5\\
    5\\
    6\\
  \end{pmatrix}
  =
  \begin{pmatrix}
    2.1825\\
    0.3905\\
    -0.0190\\
  \end{pmatrix}$$.

  \begin{align}
  => y = 2.182 + 0.39*x -0.019*x{2}
  \end{align}

  <p>
  So for (8,3) we will predict $y_1$ = 3.7968 and $y_2$ = 6.518.
  </p>
  <p>
  $|y_1 - y|$ = 0.7968
  <br>
  $|y_2 - y|$ = 3.518
  <br>
  $||y_1 - y |-|y_2 - y||$ = 2.7212   where  $|y_2 - y|$ > $|y_1 - y|$ which we were
  expecting as the least-square method is sensible to outliers.
  </p>
hint: |
  <p>
  Generalise the definition of the linear function to be under the following form
  $y = a_0 +a_1x + a_2x{2}$. Then compute the slope and the y-intercept trying to
  minimise the residual.
  </p>

comments: |
  The problem's purpose is to explose the least-square metod sensitivity to outliers,
  but also explore the least-square method using the matrix form with increased
  complexity given by the polynomial definition of the feature dependency.
